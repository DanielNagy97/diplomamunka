{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e049a3a",
   "metadata": {},
   "source": [
    "# Generatív modellek és a GAN\n",
    "\n",
    "A generatív modellek alatt olyan gépi-tanulásos architektúrákat értünk, amelyek célja, hogy új adatot állítson elő. A létrejött adatnak hasonlítnia kell a tanítómintához, vagyis a generált minták eloszlásának közelítenie kell a valós adatok eloszlását. Az ilyen modelleknek meg kell tanulnia a tanítóminta jellegzetességeit és azt is, hogy ezen jellegeket a belső reprezentációjából hogyan tudná értelmezhető formában előállítani. A generatív modellek esetében ha a modell a tanítóminta képeit generálná csupán vissza pixel-pontosan, úgy a modell elveszti célját és egyfajta ovefitting-nek tekinthetnénk a jelenséget.\n",
    "\n",
    "Egyes szerzők a minél valóságűbb eredményekre törekedtek, ehhez vagy Variational Autoencoder alapú architektúrát használtak (Ramesh et al., 2021), vagy a Generative Adverserial Network alapokon nyugvó megoldásokat (Dong et al., 2021; Reed et al., 2016; Xu et al., 2018; Zhang et al., 2018).\n",
    "\n",
    "Az Autoencoder egy olyan statisztikai elveken alapuló architektúra, amelynek célja, hogy a tanítóhalmaz jellegzetességeit feltérképezze és olyan formába kódolja a tulajdonságokat, hogy azokból az eredeti adat visszaállítható legyen. Az architektúra lényegében egy nehezen kezelhető sűrűségfüggvényt definiál, egy látens taggal, így ebben az esetben nem lehet közvetlenül optimalizálni, nem úgy mint például a pixelRNN/pixelCNN generatív modellek esetében (Oord et al., 2016), hanem az úgynevezett evidence lower bound (ELBO) mértékegységre kell optimalizálni. (Oord et al., 2017) Az architektúra két komponense az Encoder, amely előállítja a jellegvektorokat a bemenet alapján és a Decoder, amely a jellegvektorokból visszaállítja az adatot. Tehát ebben az esetben a cél az, hogy egy olyan reprezentációt készítsünk a tanítómintáról, amely alapján az teljes mértékben visszaállítható legyen. Egyfajta tömörítési folyamatnak is felfogható az encoder működése. Egy módosított változattal az encoder által létrehozandó látens tér olyan formában áll elő, amelyből véletlenszerűen is vehetünk mintákat és dekódolva teljesen új adat áll elő. Ezt a módszert Vector Quantised-Variational AutoEncoder-nek nevezték el, amely alkalmazható generatív modellként (Oord et al., 2017).\n",
    "Ha az autoencoder-t képek generálására kívánjuk felhasználni, úgy a helyreállított képeken egyfajta homályosságot figyelhetünk meg, amely a dekódolás során jelentkező információvesztésből adódik.\n",
    "\n",
    "A _Generative Adverserial Network_ (GAN), egy olyan generatív modell, amely nem a megszokott statisztikai alapokon optimalizál, mint például az Autoencoder vagy pixelRNN modellek, hanem játékelméleti megközelítést alkalmaz, így a modell tanítása is merőben másképp zajlik.\n",
    "A tanulás során két neurális hálózat versenyzik egymással: egy _generátor_, amelynek az a szerepe, hogy a tanítómintákhoz hasonló adatot generáljon a bemeneti zajból és egy _diszkriminátor_, amely egy bináris osztályozó, amely a generátor által generált adatot vizsgálja és eldönti, hogy az valódi vagy hamis.\n",
    "A tanítás során ezen két háló versenyzik egymással, együtt fejlődve. Az autoencoderhez képest a GAN-al generált képeken már nem figyelhető meg a homályosság, élesebb és fotorealisztikusabb képek generálása a fejlettebb GAN architektúrákkal megoldható.\n",
    "\n",
    "A generátor bemenete egy zajvektor, amely általában Gauss- vagy egyenletes eloszlásból állítunk elő. Ezt a zajvektort az irodalom látens térnek is nevezi, hiszen a tanítás során a modell megtanulja, hogy ezen többdimenziós tér egyes pontjaira milyen kimenetet generáljon. Vagyis ezzel lényegében kitölti a rendelkezésre álló tér tartományait a megtanult jellegzetességekkel. A betanított generátorral, optimális esetben, ezen tér bármely pontját mintavételezve a tanítóhalmazhoz hasonló adatokat generálhatunk.\n",
    "\n",
    "<img src=\"images/AEvsGAN.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa63cc",
   "metadata": {},
   "source": [
    "## GAN modell tanítása\n",
    "Ha a legegyszerűbb esetet vizsgáljuk és csak a képek rendezetlen halmazára tanítjuk a modellt, mindenféle kiegészítő információ és annotáció nélkül, akkor a tanítás a következőképpen zajlik.\n",
    "A továbbiakban a következő jelöléseket használom: legyen $D$ a _diszkriminátor_, $G$ pedig a _generátor_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a546e8",
   "metadata": {},
   "source": [
    "### Hibafüggvények\n",
    "A $G$ és $D$ hibájának számolása a bináris kereszt-entrópián alapszik.\n",
    "A bináris kereszt-entrópia hibafüggvény a következőképpen írható fel:\n",
    "$$L(\\hat y, y) = y . \\log \\hat y + (1-y). \\log (1 - \\hat y)$$\n",
    "Ahol $\\hat y$ a predikció, $y$ pedig a valós címke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d88cf3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-01 19:04:36.887596: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-01 19:04:36.887634: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "cross_entropy = BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c026973",
   "metadata": {},
   "source": [
    "#### Diszkriminátor hibafüggvénye\n",
    "\n",
    "A $G$ generátor egy $z \\in \\mathbb{R}^n, n \\geq 1$ bemeneti zajvektor alapján előállít egy generált adatot $G(z)$.\n",
    "A $D$ diszkriminátor egy bináris osztályozó, amelynek feladata, hogy az $x$ és $G(z)$ bemeneteit osztályozza.\n",
    "\n",
    "$D(x)$ esetén 1, $D(G(z))$ esetén pedig 0 címkét várunk.\n",
    "\n",
    "A hibafüggvény számolása két lépésben történik a kétféle bemenet miatt:\n",
    "\n",
    "$D(x)$-re nézve a kereszt-entrópia a következő:\n",
    "$$L(D(x), 1) = 1.\\ln D(x) + (1 - 1).\\ln(1 - D(x))$$\n",
    "$$L(D(x), 1) = \\ln D(x)$$\n",
    "Vagyis jelen esetben $D$-nek a $\\ln(D(x))$-et kell maximalizálnia.\n",
    "\n",
    "$D(G(z))$-re nézve a kereszt-entrópia a következő:\n",
    "$$L(D(G(z)), 0) = 0.\\ln D(G(z)) + (1 - 0).\\ln(1 - D(G(z)))$$\n",
    "$$L(D(G(z)), 0) = \\ln(1- D(G(z)))$$\n",
    "Vagyis a $\\ln(1 - D(G(z)))$-t kell maximalizálnia.\n",
    "\n",
    "Egyetlen mintára a hibafüggvény a következőképpen néz ki:\n",
    "$$\\max V(D) = \\ln D(x) + \\ln(1 - D(G(z))$$\n",
    "\n",
    "Batch-ra nézve:\n",
    "$$\\max V(D) = \\mathbb{E}_{x \\sim P(x)} \\left[\\ln D(x) \\right] + \\mathbb{E}_{z \\sim P(z)} \\left[\\ln(1 - D(G(z))) \\right]$$\n",
    "\n",
    "Ahol a $P(x)$ a valószínűségi eloszlása a tanítóhalmaznak, $P(z)$ a valószínűségi eloszlása a $z$ zajvektornak. (látens tér)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0089898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    \n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54caea8c",
   "metadata": {},
   "source": [
    "#### Generátor hibafüggvénye\n",
    "\n",
    "A $G$ generátor feladata az, hogy megtévessze a $D$ diszkriminátort azáltal, hogy a tanítóhalmazhoz hasonló adatokat generáljon.\n",
    "Vagyis a $G$ érdeke az, hogy a $D(G(z))$ 1-es címkét kapjon 0 helyett.\n",
    "\n",
    "Tehát a bináris keresztentrópia egy mintára:\n",
    "$$L(D(G(z)), 0) = \\ln(1 - D(G(z))$$\n",
    "$D$ minimalizálni kívánja a $D(G(z))$-t, míg a $G$ maximalizálni szándékozik azt.\n",
    "\n",
    "A $G$ a tanítás során sosem fog valódi adatot látni, de a teljesség kedvééert a hibafüggvénye a következőképpen írható fel (Csak a második kifejezést minimalizálja valójában):\n",
    "$$\\min V(G) = \\mathbb{E}_{x \\sim P(x)} \\left[\\ln D(x) \\right] + \\mathbb{E}_{z \\sim P(z)} \\left[\\ln(1 - D(G(z))) \\right]$$\n",
    "\n",
    "Vagyis a GAN hálózat tanítása során $D$ és $G$ egy minimax játékot játszanak a $V(G, D)$ értékfüggvénnyel.\n",
    "$$\\min_{G}\\max_{D}V(D, G) =  \\mathbb{E}_{x \\sim P(x)} \\left[\\ln D(x) \\right] + \\mathbb{E}_{z \\sim P(z)} \\left[\\ln(1 - D(G(z))) \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0017e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550e98d1",
   "metadata": {},
   "source": [
    "TODO: Számolás bemutatása egy példán keresztül, `from_logits=True` jelentősége (lineáris kimenetre sigmoidot rak)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4da76d",
   "metadata": {},
   "source": [
    "### Optimalizáló módszer\n",
    "\n",
    "A GAN modell súlyait a sztochasztikus gradiens algoritmussal szokás frissíteni.\n",
    "\n",
    "A Generátort és a Diszkriminátort a hibafüggvények alapján kiszámolt gradiensek alapján külön-külön kell tanítani. Különféle szerzők különböző optimalizáló módszerek használatát javasolják, az Adam és az RMSProp a két legnépszerűbb módszer.\n",
    "\n",
    "TODO: Optimalizáló módszerek bemtatása!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d93c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "generator_optimizer = Adam(1e-4)\n",
    "discriminator_optimizer = Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f242c1",
   "metadata": {},
   "source": [
    "### Tanítási lépés\n",
    "%% GAN 2014 cikkből\n",
    "\n",
    "A GAN hálózat egy tanítási lépése a következő lépésekből áll:\n",
    "\n",
    "Legyen $m$ a minibatch elemszáma $m \\in \\mathbb{N}$\n",
    "\n",
    "1. Hozzunk létre $m$ darab zajmintát $(z_1, \\ldots, z_m)$ gauss eloszlásból $P_g(z)$.\n",
    "2. A tanítóhalmazból emeljük ki a soronkövetkező $m$ darab tanítómintát (képet), és ezt jelöljük $(x_1, \\ldots, x_m)$-el $P_{\\text{data}}(x)$\n",
    "3. Frissítsük a $D$ diszkriminátort a sztochasztikus gradiens emelkedésével (?? tükörfordítás)\n",
    "$$ \\nabla \\theta_d \\frac{1}{m} \\sum_{i=1}^{m} \\left[\\log D(x_i) + \\log(1 - D(G(z_i))) \\right]$$\n",
    "4. Frissítsük a $G$ generátort a sztochasztikus gradiens lejtésével (?? tükörfordítás)\n",
    "$$ \\nabla \\theta_d \\frac{1}{m} \\sum_{i=1}^{m} \\log(1 - D(G(z_i)))$$\n",
    "\n",
    "Természetesen nem egyszerre tanítjuk a GAN részeit. Az eredeti cikkben is javaslatot tesznek arra, hogy a $D$-t esetleg több lépésben is lehetne tanítani, majd a $G$-t egyetlen lépésben frissíteni.\n",
    "Különböző tanítási stratégiákban ez is egy szabad paraméter lehet. Számomra megfelelő volt az 1:1-es tanítási lépés alkalmazása is... (Esetleg lehetne mérni valahogy, hogy van-e számottevő különbség...)\n",
    "A tanítás hossza természetesen függ az adathalmaztól és annak méretétől, a minibatch mérettől, a modellben található paraméterektől és az optimalizáló függvénytől."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dcf0414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = random.normal([batch_size, latent_dim])\n",
    "\n",
    "    with GradientTape() as gen_tape, GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss,\n",
    "                                               generator.trainable_variables)\n",
    "\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss,\n",
    "                                                    discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator,\n",
    "                                            generator.trainable_variables))\n",
    "\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator,\n",
    "                                                discriminator.trainable_variables))\n",
    "    return (gen_loss, disc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f1e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        for (batch, image_batch) in enumerate(dataset):\n",
    "            gen_loss, disc_loss = train_step(image_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad24731",
   "metadata": {},
   "source": [
    "## GAN teljesítményének mérese, technikák\n",
    "\n",
    "TODO: Teljesítmény mérésének nehézségei, javaslatok összefoglalása, bemutatása\n",
    "- Inception Score (salimans2016improved, barratt2018note)\n",
    "- Fréchet Inception Distance (heusel2017gans)\n",
    "\n",
    "Összefoglaló táblázat, saját modellek eredményei, ábrák"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
