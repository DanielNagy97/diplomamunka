{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e049a3a",
   "metadata": {},
   "source": [
    "# Generatív modellek és a GAN\n",
    "\n",
    "A _Generative Adverserial Network_ (GAN), egy olyan generatív modell, amely nem a megszokott statisztikai alapokon optimalizál, mint például az Autoencoder vagy pixelRNN modellek, hanem játékelméleti megközelítést alkalmaz. (Az generatív modellekről lehetne írni esetleg egy áttekintést, hogy miért éppen a GAN-ra esett a választás mondjuk a VAE-val szemben?)\n",
    "A tanulás során két neurális hálózat versenyzik egymással: egy _generátor_, amelynek az a szerepe, hogy a tanítómintákhoz hasonló adatot generáljon a bemeneti zajból és egy _diszkriminátor_, amely egy bináris osztályozó, amely a generátor által generált adatot vizsgálja és eldönti, hogy az valódi vagy hamis.\n",
    "A tanítás során ezen két háló versenyzik egymással, együtt fejlődve.\n",
    "A generátor bemenete egy zajvektor, amely általában Gauss- vagy egyenletes eloszlásból állítunk elő. Ezt a zajvektort az irodalom látens térnek is nevezi, hiszen a tanítás során a modell megtanulja, hogy ezen többdimenziós tér egyes pontjaira milyen kimenetet generáljon. Vagyis ezzel lényegében kitölti a rendelkezésre álló tér tartományait a megtanult jellegzetességekkel. A generátorral ezen tér bármely pontját mintavételezve a tanítóhalmazhoz hasonló adatokat generálhatunk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa63cc",
   "metadata": {},
   "source": [
    "## Modell tanítása\n",
    "Ha a legegyszerűbb esetet vizsgáljuk és csak a képek rendezetlen halmazára tanítjuk a modellt, mindenféle kiegészítő információ és annotáció nélkül, akkor a tanítás a következőképpen zajlik.\n",
    "A továbbiakban a következő jelöléseket használom: legyen $D$ a _diszkriminátor_, $G$ pedig a _generátor_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a546e8",
   "metadata": {},
   "source": [
    "### Hibafüggvények\n",
    "A $G$ és $D$ hibájának számolása a bináris kereszt-entrópián alapszik.\n",
    "A bináris kereszt-entrópia hibafüggvény a következőképpen írható fel:\n",
    "$$L(\\hat y, y) = y . \\log \\hat y + (1-y). \\log (1 - \\hat y)$$\n",
    "Ahol $\\hat y$ a predikció, $y$ pedig a valós címke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d88cf3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-01 19:04:36.887596: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-01 19:04:36.887634: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "cross_entropy = BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c026973",
   "metadata": {},
   "source": [
    "#### Diszkriminátor hibafüggvénye\n",
    "\n",
    "A $G$ generátor egy $z \\in \\mathbb{R}^n, n \\geq 1$ bemeneti zajvektor alapján előállít egy generált adatot $G(z)$.\n",
    "A $D$ diszkriminátor egy bináris osztályozó, amelynek feladata, hogy az $x$ és $G(z)$ bemeneteit osztályozza.\n",
    "\n",
    "$D(x)$ esetén 1, $D(G(z))$ esetén pedig 0 címkét várunk.\n",
    "\n",
    "A hibafüggvény számolása két lépésben történik a kétféle bemenet miatt:\n",
    "\n",
    "$D(x)$-re nézve a kereszt-entrópia a következő:\n",
    "$$L(D(x), 1) = 1.\\ln D(x) + (1 - 1).\\ln(1 - D(x))$$\n",
    "$$L(D(x), 1) = \\ln D(x)$$\n",
    "Vagyis jelen esetben $D$-nek a $\\ln(D(x))$-et kell maximalizálnia.\n",
    "\n",
    "$D(G(z))$-re nézve a kereszt-entrópia a következő:\n",
    "$$L(D(G(z)), 0) = 0.\\ln D(G(z)) + (1 - 0).\\ln(1 - D(G(z)))$$\n",
    "$$L(D(G(z)), 0) = \\ln(1- D(G(z)))$$\n",
    "Vagyis a $\\ln(1 - D(G(z)))$-t kell maximalizálnia.\n",
    "\n",
    "Egyetlen mintára a hibafüggvény a következőképpen néz ki:\n",
    "$$\\max V(D) = \\ln D(x) + \\ln(1 - D(G(z))$$\n",
    "\n",
    "Batch-ra nézve:\n",
    "$$\\max V(D) = \\mathbb{E}_{x \\sim P(x)} \\left[\\ln D(x) \\right] + \\mathbb{E}_{z \\sim P(z)} \\left[\\ln(1 - D(G(z))) \\right]$$\n",
    "\n",
    "Ahol a $P(x)$ a valószínűségi eloszlása a tanítóhalmaznak, $P(z)$ a valószínűségi eloszlása a $z$ zajvektornak. (látens tér)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0089898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    \n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54caea8c",
   "metadata": {},
   "source": [
    "#### Generátor hibafüggvénye\n",
    "\n",
    "A $G$ generátor feladata az, hogy megtévessze a $D$ diszkriminátort azáltal, hogy a tanítóhalmazhoz hasonló adatokat generáljon.\n",
    "Vagyis a $G$ érdeke az, hogy a $D(G(z))$ 1-es címkét kapjon 0 helyett.\n",
    "\n",
    "Tehát a bináris keresztentrópia egy mintára:\n",
    "$$L(D(G(z)), 0) = \\ln(1 - D(G(z))$$\n",
    "$D$ minimalizálni kívánja a $D(G(z))$-t, míg a $G$ maximalizálni szándékozik azt.\n",
    "\n",
    "A $G$ a tanítás során sosem fog valódi adatot látni, de a teljesség kedvééert a hibafüggvénye a következőképpen írható fel (Csak a második kifejezést minimalizálja valójában):\n",
    "$$\\min V(G) = \\mathbb{E}_{x \\sim P(x)} \\left[\\ln D(x) \\right] + \\mathbb{E}_{z \\sim P(z)} \\left[\\ln(1 - D(G(z))) \\right]$$\n",
    "\n",
    "Vagyis a GAN hálózat tanítása során $D$ és $G$ egy minimax játékot játszanak a $V(G, D)$ értékfüggvénnyel.\n",
    "$$\\min_{G}\\max_{D}V(D, G) =  \\mathbb{E}_{x \\sim P(x)} \\left[\\ln D(x) \\right] + \\mathbb{E}_{z \\sim P(z)} \\left[\\ln(1 - D(G(z))) \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0017e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550e98d1",
   "metadata": {},
   "source": [
    "TODO: Számolás bemutatása egy példán keresztül, `from_logits=True` jelentősége (lineáris kimenetre sigmoidot rak)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4da76d",
   "metadata": {},
   "source": [
    "### Optimalizáló módszer\n",
    "\n",
    "A GAN modell súlyait a sztochasztikus gradiens algoritmussal szokás frissíteni.\n",
    "\n",
    "A Generátort és a Diszkriminátort a hibafüggvények alapján kiszámolt gradiensek alapján külön-külön kell tanítani. Különféle szerzők különböző optimalizáló módszerek használatát javasolják, az Adam és az RMSProp a két legnépszerűbb módszer.\n",
    "\n",
    "TODO: Optimalizáló módszerek bemtatása!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d93c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "generator_optimizer = Adam(1e-4)\n",
    "discriminator_optimizer = Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f242c1",
   "metadata": {},
   "source": [
    "### Tanítási lépés\n",
    "%% GAN 2014 cikkből\n",
    "\n",
    "A GAN hálózat egy tanítási lépése a következő lépésekből áll:\n",
    "\n",
    "Legyen $m$ a minibatch elemszáma $m \\in \\mathbb{N}$\n",
    "\n",
    "1. Hozzunk létre $m$ darab zajmintát $(z_1, \\ldots, z_m)$ gauss eloszlásból $P_g(z)$.\n",
    "2. A tanítóhalmazból emeljük ki a soronkövetkező $m$ darab tanítómintát (képet), és ezt jelöljük $(x_1, \\ldots, x_m)$-el $P_{\\text{data}}(x)$\n",
    "3. Frissítsük a $D$ diszkriminátort a sztochasztikus gradiens emelkedésével (?? tükörfordítás)\n",
    "$$ \\nabla \\theta_d \\frac{1}{m} \\sum_{i=1}^{m} \\left[\\log D(x_i) + \\log(1 - D(G(z_i))) \\right]$$\n",
    "4. Frissítsük a $G$ generátort a sztochasztikus gradiens lejtésével (?? tükörfordítás)\n",
    "$$ \\nabla \\theta_d \\frac{1}{m} \\sum_{i=1}^{m} \\log(1 - D(G(z_i)))$$\n",
    "\n",
    "Természetesen nem egyszerre tanítjuk a GAN részeit. Az eredeti cikkben is javaslatot tesznek arra, hogy a $D$-t esetleg több lépésben is lehetne tanítani, majd a $G$-t egyetlen lépésben frissíteni.\n",
    "Különböző tanítási stratégiákban ez is egy szabad paraméter lehet. Számomra megfelelő volt az 1:1-es tanítási lépés alkalmazása is... (Esetleg lehetne mérni valahogy, hogy van-e számottevő különbség...)\n",
    "A tanítás hossza természetesen függ az adathalmaztól és annak méretétől, a minibatch mérettől, a modellben található paraméterektől és az optimalizáló függvénytől."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dcf0414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = random.normal([batch_size, latent_dim])\n",
    "\n",
    "    with GradientTape() as gen_tape, GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss,\n",
    "                                               generator.trainable_variables)\n",
    "\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss,\n",
    "                                                    discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator,\n",
    "                                            generator.trainable_variables))\n",
    "\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator,\n",
    "                                                discriminator.trainable_variables))\n",
    "    return (gen_loss, disc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f1e55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        for (batch, image_batch) in enumerate(dataset):\n",
    "            gen_loss, disc_loss = train_step(image_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad24731",
   "metadata": {},
   "source": [
    "## GAN teljesítményének mérese, technikák\n",
    "\n",
    "TODO: Teljesítmény mérésének nehézségei, javaslatok összefoglalása, bemutatása\n",
    "- Inception Score (salimans2016improved, barratt2018note)\n",
    "- Fréchet Inception Distance (heusel2017gans)\n",
    "\n",
    "Összefoglaló táblázat, saját modellek eredményei, ábrák"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
