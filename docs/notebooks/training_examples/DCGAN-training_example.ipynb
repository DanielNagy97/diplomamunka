{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T11:33:48.873901Z",
     "iopub.status.busy": "2022-02-28T11:33:48.873312Z",
     "iopub.status.idle": "2022-02-28T11:33:55.056215Z",
     "shell.execute_reply": "2022-02-28T11:33:55.055175Z",
     "shell.execute_reply.started": "2022-02-28T11:33:48.873762Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T11:33:55.058593Z",
     "iopub.status.busy": "2022-02-28T11:33:55.058114Z",
     "iopub.status.idle": "2022-02-28T11:34:13.071490Z",
     "shell.execute_reply": "2022-02-28T11:34:13.070559Z",
     "shell.execute_reply.started": "2022-02-28T11:33:55.058535Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "number_of_examples = 16\n",
    "batch_size = 16\n",
    "latent_dim = 100\n",
    "image_size = (64, 64) # h x w\n",
    "\n",
    "seed = tf.random.normal([number_of_examples, latent_dim])\n",
    "\n",
    "data_dir = r'../input/animal-faces/afhq/train/'\n",
    "\n",
    "dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir, label_mode=None, image_size=image_size, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T11:34:13.073579Z",
     "iopub.status.busy": "2022-02-28T11:34:13.073268Z",
     "iopub.status.idle": "2022-02-28T11:34:13.102029Z",
     "shell.execute_reply": "2022-02-28T11:34:13.101199Z",
     "shell.execute_reply.started": "2022-02-28T11:34:13.073520Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: (x - 127.5) / 127.5) # Normalizing to -1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T11:34:13.104763Z",
     "iopub.status.busy": "2022-02-28T11:34:13.104332Z",
     "iopub.status.idle": "2022-02-28T11:34:13.114576Z",
     "shell.execute_reply": "2022-02-28T11:34:13.113583Z",
     "shell.execute_reply.started": "2022-02-28T11:34:13.104722Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_upsampling_unit(model,\n",
    "                        filters, kernel_size, strides, padding):\n",
    "    model.add(\n",
    "        keras.layers.Conv2DTranspose(\n",
    "            filters=filters, kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding, activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\"\n",
    "        )\n",
    "    )\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "def make_generator_model(latent_dim):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Reshape((1, 1, 100), input_shape=[latent_dim]))\n",
    "\n",
    "    add_upsampling_unit(model, 512, 4, (1, 1), 'valid')\n",
    "\n",
    "    add_upsampling_unit(model, 256, 4, (2, 2), 'same')\n",
    "\n",
    "    add_upsampling_unit(model, 128, 4, (2, 2), 'same')\n",
    "\n",
    "    add_upsampling_unit(model, 64, 4, (2, 2), 'same')\n",
    "\n",
    "    model.add(keras.layers.Conv2DTranspose(filters=3, kernel_size=4,\n",
    "                                           strides=(2, 2), padding='same'))\n",
    "    model.add(keras.layers.Activation(\"tanh\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T11:34:13.117082Z",
     "iopub.status.busy": "2022-02-28T11:34:13.116386Z",
     "iopub.status.idle": "2022-02-28T11:34:20.072299Z",
     "shell.execute_reply": "2022-02-28T11:34:20.071426Z",
     "shell.execute_reply.started": "2022-02-28T11:34:13.117036Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = make_generator_model(latent_dim)\n",
    "\n",
    "noise = tf.random.normal([1, latent_dim])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow((generated_image[0].numpy()*127.5+127.5).astype(\"uint32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T11:34:20.074432Z",
     "iopub.status.busy": "2022-02-28T11:34:20.073927Z",
     "iopub.status.idle": "2022-02-28T11:34:20.086716Z",
     "shell.execute_reply": "2022-02-28T11:34:20.085631Z",
     "shell.execute_reply.started": "2022-02-28T11:34:20.074359Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_downsampling_unit(model, filters,\n",
    "                          kernel_size, strides, padding):\n",
    "    model.add(\n",
    "        keras.layers.Conv2D(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding, activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\"\n",
    "        )\n",
    "    )\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(\n",
    "        keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=4,\n",
    "            strides=2,\n",
    "            input_shape=(64, 64, 3),\n",
    "            padding=\"same\", activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\"\n",
    "        )\n",
    "    )\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "    add_downsampling_unit(model, filters=128,\n",
    "                          kernel_size=4, strides=2, padding=\"same\")\n",
    "\n",
    "    add_downsampling_unit(model, filters=256,\n",
    "                          kernel_size=4, strides=2, padding=\"same\")\n",
    "\n",
    "    add_downsampling_unit(model, filters=512,\n",
    "                          kernel_size=4, strides=2, padding=\"same\")\n",
    "\n",
    "    model.add(\n",
    "        keras.layers.Conv2D(\n",
    "            filters=100,\n",
    "            kernel_size=4,\n",
    "            strides=1,\n",
    "            padding=\"valid\",\n",
    "            activation=\"relu\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    # model.add(Activation(\"sigmoid\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T11:34:20.089012Z",
     "iopub.status.busy": "2022-02-28T11:34:20.088330Z",
     "iopub.status.idle": "2022-02-28T11:34:20.361483Z",
     "shell.execute_reply": "2022-02-28T11:34:20.360391Z",
     "shell.execute_reply.started": "2022-02-28T11:34:20.088965Z"
    }
   },
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T11:34:20.363802Z",
     "iopub.status.busy": "2022-02-28T11:34:20.363361Z",
     "iopub.status.idle": "2022-02-28T11:34:20.371722Z",
     "shell.execute_reply": "2022-02-28T11:34:20.370767Z",
     "shell.execute_reply.started": "2022-02-28T11:34:20.363760Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    \n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T11:34:20.374455Z",
     "iopub.status.busy": "2022-02-28T11:34:20.373377Z",
     "iopub.status.idle": "2022-02-28T11:34:20.387512Z",
     "shell.execute_reply": "2022-02-28T11:34:20.386523Z",
     "shell.execute_reply.started": "2022-02-28T11:34:20.374397Z"
    }
   },
   "outputs": [],
   "source": [
    "generator_optimizer = keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = keras.optimizers.Adam(4e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T11:34:20.392208Z",
     "iopub.status.busy": "2022-02-28T11:34:20.391810Z",
     "iopub.status.idle": "2022-02-28T11:34:20.399384Z",
     "shell.execute_reply": "2022-02-28T11:34:20.398420Z",
     "shell.execute_reply.started": "2022-02-28T11:34:20.392160Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(\n",
    "    generator_optimizer=generator_optimizer,\n",
    "    discriminator_optimizer=discriminator_optimizer,\n",
    "    generator=generator,\n",
    "    discriminator=discriminator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T11:34:20.401884Z",
     "iopub.status.busy": "2022-02-28T11:34:20.401510Z",
     "iopub.status.idle": "2022-02-28T11:34:20.413907Z",
     "shell.execute_reply": "2022-02-28T11:34:20.412707Z",
     "shell.execute_reply.started": "2022-02-28T11:34:20.401835Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([batch_size, latent_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(\n",
    "        gen_loss,\n",
    "        generator.trainable_variables\n",
    "    )\n",
    "\n",
    "    gradients_of_discriminator = disc_tape.gradient(\n",
    "        disc_loss,\n",
    "        discriminator.trainable_variables\n",
    "    )\n",
    "\n",
    "    generator_optimizer.apply_gradients(\n",
    "        zip(gradients_of_generator,\n",
    "            generator.trainable_variables)\n",
    "        )\n",
    "    discriminator_optimizer.apply_gradients(\n",
    "        zip(gradients_of_discriminator,\n",
    "            discriminator.trainable_variables)\n",
    "        )\n",
    "\n",
    "    return (gen_loss, disc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T11:34:20.416251Z",
     "iopub.status.busy": "2022-02-28T11:34:20.415826Z",
     "iopub.status.idle": "2022-02-28T11:34:20.428994Z",
     "shell.execute_reply": "2022-02-28T11:34:20.427738Z",
     "shell.execute_reply.started": "2022-02-28T11:34:20.416193Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"epochs\"):\n",
    "    os.mkdir(\"epochs\")\n",
    "\n",
    "def plot_grid_of_images(images, epoch):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow((images[i].numpy() * 127.5 + 127.5).astype(\"uint32\"))\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.savefig('epochs/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T11:34:20.431596Z",
     "iopub.status.busy": "2022-02-28T11:34:20.431033Z",
     "iopub.status.idle": "2022-02-28T11:34:20.449230Z",
     "shell.execute_reply": "2022-02-28T11:34:20.447743Z",
     "shell.execute_reply.started": "2022-02-28T11:34:20.431504Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    generator_losses = np.empty((0, 0), dtype=float)\n",
    "    discriminator_losses = np.empty((0, 0), dtype=float)\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        batch_generator_losses = np.empty((0, 0), dtype=float)\n",
    "        batch_discriminator_losses = np.empty((0, 0), dtype=float)\n",
    "        for (batch, image_batch) in enumerate(dataset):\n",
    "            gen_loss, disc_loss = train_step(image_batch)\n",
    "            \n",
    "            if batch % 100 == 0:\n",
    "                average_batch_loss =\\\n",
    "                   gen_loss.numpy()/int(image_batch.shape[1])\n",
    "                print(f\"\"\"Epoch {epoch+1}\n",
    "                        Batch {batch} Loss {average_batch_loss:.4f}\"\"\")\n",
    "\n",
    "            batch_generator_losses = np.append(batch_generator_losses, gen_loss)\n",
    "            batch_discriminator_losses = np.append(batch_discriminator_losses, disc_loss)\n",
    "            \n",
    "        if generator_losses.shape == (0, 0):\n",
    "            generator_losses = batch_generator_losses\n",
    "            discriminator_losses = batch_discriminator_losses\n",
    "        else:\n",
    "            generator_losses = np.vstack(\n",
    "                [generator_losses, batch_generator_losses]\n",
    "            )\n",
    "            discriminator_losses = np.vstack(\n",
    "                [discriminator_losses, batch_discriminator_losses]\n",
    "            )\n",
    "            \n",
    "        # Saving the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            \n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "        example_images = generator(seed, training=False)\n",
    "        plot_grid_of_images(example_images, epoch)\n",
    "        \n",
    "\n",
    "    # Generating after the final epoch\n",
    "    example_images = generator(seed, training=False)\n",
    "    plot_grid_of_images(example_images, epoch)\n",
    "    \n",
    "    return (generator_losses, discriminator_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-28T11:34:20.451759Z",
     "iopub.status.busy": "2022-02-28T11:34:20.451337Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "(generator_losses, discriminator_losses) = train(dataset, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = generator_losses[:epochs].mean(axis=1) / batch_size\n",
    "disc = discriminator_losses[:epochs].mean(axis=1) / batch_size\n",
    "\n",
    "fig_1 = plt.figure(figsize=(8, 4), dpi=100)\n",
    "ax = fig_1.add_axes([0, 0, 1, 1])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('DCGAN hibaértékei az első 50 epoch alatt')\n",
    "ax.plot(np.linspace(1, epochs, epochs), gen, label='Generator')\n",
    "ax.plot(np.linspace(1, epochs, epochs), disc, label='Discriminator')\n",
    "ax.legend(loc=0)\n",
    "\n",
    "ax.grid(True, color='0.6', dashes=(5, 2, 1, 2))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(0.05))\n",
    "\n",
    "x_ticks = np.arange(0, epochs+1, 5)\n",
    "x_ticks[0] = 1\n",
    "ax.set_xticks(x_ticks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
