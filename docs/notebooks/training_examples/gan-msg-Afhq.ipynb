{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T08:00:51.526412Z",
     "iopub.status.busy": "2022-05-01T08:00:51.526059Z",
     "iopub.status.idle": "2022-05-01T08:00:55.736454Z",
     "shell.execute_reply": "2022-05-01T08:00:55.735673Z",
     "shell.execute_reply.started": "2022-05-01T08:00:51.526329Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T08:00:55.738119Z",
     "iopub.status.busy": "2022-05-01T08:00:55.737869Z",
     "iopub.status.idle": "2022-05-01T08:01:18.028025Z",
     "shell.execute_reply": "2022-05-01T08:01:18.027288Z",
     "shell.execute_reply.started": "2022-05-01T08:00:55.738086Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "number_of_examples = 8\n",
    "batch_size = 16\n",
    "latent_dim = 100\n",
    "image_size = (64, 64) # h x w\n",
    "\n",
    "seed = tf.random.normal([number_of_examples, latent_dim])\n",
    "\n",
    "data_dir = r'../input/animal-faces/afhq/train/'\n",
    "\n",
    "dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir, label_mode=None, image_size=image_size, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T08:01:18.031057Z",
     "iopub.status.busy": "2022-05-01T08:01:18.029238Z",
     "iopub.status.idle": "2022-05-01T08:01:18.054713Z",
     "shell.execute_reply": "2022-05-01T08:01:18.05405Z",
     "shell.execute_reply.started": "2022-05-01T08:01:18.031025Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: (x - 127.5) / 127.5) # Normalizing to -1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T08:01:18.056959Z",
     "iopub.status.busy": "2022-05-01T08:01:18.056728Z",
     "iopub.status.idle": "2022-05-01T08:01:18.073394Z",
     "shell.execute_reply": "2022-05-01T08:01:18.072678Z",
     "shell.execute_reply.started": "2022-05-01T08:01:18.056927Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_generator_model(latent_dim):\n",
    "    visible = keras.layers.Input(shape=[latent_dim])\n",
    "    hidden = keras.layers.Reshape((1, 1, 100))(visible)\n",
    "\n",
    "    hidden = keras.layers.Conv2DTranspose(filters=512, kernel_size=4, strides=(1, 1),\n",
    "                                          padding='valid', activation=\"relu\")(hidden)\n",
    "    hidden = keras.layers.BatchNormalization()(hidden)\n",
    "    out4x4 = hidden\n",
    "    out4x4 = keras.layers.Conv2D(filters=3, kernel_size=4, strides=(1, 1), padding='same')(out4x4)\n",
    "    out4x4 = keras.layers.Activation(\"tanh\")(out4x4)\n",
    "\n",
    "\n",
    "    hidden = keras.layers.Conv2DTranspose(filters=256, kernel_size=4, strides=(2, 2),\n",
    "                                          padding='same', activation=\"relu\", kernel_initializer=\"he_normal\")(hidden)\n",
    "    hidden = keras.layers.BatchNormalization()(hidden)\n",
    "    out8x8 = hidden\n",
    "    out8x8 = keras.layers.Conv2D(filters=3, kernel_size=4, strides=(1, 1), padding='same')(out8x8)\n",
    "    out8x8 = keras.layers.Activation(\"tanh\")(out8x8)\n",
    "\n",
    "\n",
    "    hidden = keras.layers.Conv2DTranspose(filters=128, kernel_size=4, strides=(2, 2),\n",
    "                                          padding='same', activation=\"relu\", kernel_initializer=\"he_normal\")(hidden)\n",
    "    hidden = keras.layers.BatchNormalization()(hidden)\n",
    "    out16x16 = hidden\n",
    "    out16x16 = keras.layers.Conv2D(filters=3, kernel_size=4, strides=(1, 1), padding='same')(out16x16)\n",
    "    out16x16 = keras.layers.Activation(\"tanh\")(out16x16)\n",
    "\n",
    "\n",
    "    hidden = keras.layers.Conv2DTranspose(filters=64, kernel_size=4, strides=(2, 2),\n",
    "                                          padding='same', activation=\"relu\", kernel_initializer=\"he_normal\")(hidden)\n",
    "    hidden = keras.layers.BatchNormalization()(hidden)\n",
    "    out32x32 = hidden\n",
    "    out32x32 = keras.layers.Conv2D(filters=3, kernel_size=4, strides=(1, 1), padding='same')(out32x32)\n",
    "    out32x32 = keras.layers.Activation(\"tanh\")(out32x32)\n",
    "\n",
    "\n",
    "    hidden = keras.layers.Conv2DTranspose(filters=64, kernel_size=4, strides=(2, 2),\n",
    "                                          padding='same', activation=\"relu\", kernel_initializer=\"he_normal\")(hidden)\n",
    "    hidden = keras.layers.BatchNormalization()(hidden)\n",
    "    hidden = keras.layers.Conv2D(filters=3, kernel_size=4, strides=(1, 1), padding='same')(hidden)\n",
    "    out64x64 = keras.layers.Activation(\"tanh\")(hidden)\n",
    "    model = keras.models.Model(inputs=visible, outputs=[out64x64, out32x32, out16x16, out8x8, out4x4])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T08:01:18.07484Z",
     "iopub.status.busy": "2022-05-01T08:01:18.074581Z",
     "iopub.status.idle": "2022-05-01T08:01:24.590956Z",
     "shell.execute_reply": "2022-05-01T08:01:24.590249Z",
     "shell.execute_reply.started": "2022-05-01T08:01:18.074807Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = make_generator_model(latent_dim)\n",
    "\n",
    "noise = tf.random.normal([1, latent_dim])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow((generated_image[0][0].numpy()*127.5+127.5).astype(\"uint32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T08:01:24.592926Z",
     "iopub.status.busy": "2022-05-01T08:01:24.592335Z",
     "iopub.status.idle": "2022-05-01T08:01:24.612055Z",
     "shell.execute_reply": "2022-05-01T08:01:24.611287Z",
     "shell.execute_reply.started": "2022-05-01T08:01:24.592889Z"
    }
   },
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T08:01:24.613522Z",
     "iopub.status.busy": "2022-05-01T08:01:24.613176Z",
     "iopub.status.idle": "2022-05-01T08:01:25.558812Z",
     "shell.execute_reply": "2022-05-01T08:01:25.557295Z",
     "shell.execute_reply.started": "2022-05-01T08:01:24.613487Z"
    }
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(generator, dpi=100, show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T08:01:25.56078Z",
     "iopub.status.busy": "2022-05-01T08:01:25.560537Z",
     "iopub.status.idle": "2022-05-01T08:01:25.576662Z",
     "shell.execute_reply": "2022-05-01T08:01:25.575967Z",
     "shell.execute_reply.started": "2022-05-01T08:01:25.560748Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    input1 = keras.layers.Input(shape=(64, 64, 3))\n",
    "    hidden = keras.layers.Conv2D(filters=64, kernel_size=4, strides=2, padding=\"same\",\n",
    "                                 activation=\"relu\", kernel_initializer=\"he_normal\")(input1)\n",
    "    hidden = keras.layers.BatchNormalization()(hidden)\n",
    "\n",
    "    input2 = keras.layers.Input(shape=(32, 32, 3))\n",
    "    inputfeature2 = keras.layers.Conv2D(64, 3, 1, padding=\"same\")(input2)\n",
    "    hidden = keras.layers.Concatenate()([hidden, inputfeature2])\n",
    "    hidden = keras.layers.Conv2D(filters=128, kernel_size=4, strides=2, padding=\"same\",\n",
    "                                 activation=\"relu\", kernel_initializer=\"he_normal\")(hidden)\n",
    "    hidden = keras.layers.BatchNormalization()(hidden)\n",
    "\n",
    "    input3 = keras.layers.Input(shape=(16, 16, 3))\n",
    "    inputfeature3 = keras.layers.Conv2D(128, 3, 1, padding=\"same\")(input3)\n",
    "    hidden = keras.layers.Concatenate()([hidden, inputfeature3])\n",
    "    hidden = keras.layers.Conv2D(filters=256, kernel_size=4, strides=2, padding=\"same\",\n",
    "                                 activation=\"relu\", kernel_initializer=\"he_normal\")(hidden)\n",
    "    hidden = keras.layers.BatchNormalization()(hidden)\n",
    "\n",
    "    input4 = keras.layers.Input(shape=(8, 8, 3))\n",
    "    inputfeature4 = keras.layers.Conv2D(256, 3, 1, padding=\"same\")(input4)\n",
    "    hidden = keras.layers.Concatenate()([hidden, inputfeature4])\n",
    "    hidden = keras.layers.Conv2D(filters=512, kernel_size=4, strides=2, padding=\"same\",\n",
    "                                 activation=\"relu\", kernel_initializer=\"he_normal\")(hidden)\n",
    "    hidden = keras.layers.BatchNormalization()(hidden)\n",
    "\n",
    "    input5 = keras.layers.Input(shape=(4, 4, 3))\n",
    "    inputfeature5 = keras.layers.Conv2D(512, 3, 1, padding=\"same\")(input5)\n",
    "    hidden = keras.layers.Concatenate()([hidden, inputfeature5])\n",
    "\n",
    "    hidden = keras.layers.Conv2D(filters=100, kernel_size=4, strides=1, padding=\"valid\", activation=\"relu\")(hidden)\n",
    "\n",
    "    out = keras.layers.Dense(1)(hidden)    \n",
    "\n",
    "    model = keras.models.Model(inputs=[input1, input2, input3, input4, input5], outputs=[out])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T08:01:25.579562Z",
     "iopub.status.busy": "2022-05-01T08:01:25.577893Z",
     "iopub.status.idle": "2022-05-01T08:01:25.777892Z",
     "shell.execute_reply": "2022-05-01T08:01:25.777119Z",
     "shell.execute_reply.started": "2022-05-01T08:01:25.579524Z"
    }
   },
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image, training=False)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T08:01:25.781436Z",
     "iopub.status.busy": "2022-05-01T08:01:25.781193Z",
     "iopub.status.idle": "2022-05-01T08:01:25.797884Z",
     "shell.execute_reply": "2022-05-01T08:01:25.797185Z",
     "shell.execute_reply.started": "2022-05-01T08:01:25.78138Z"
    }
   },
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T08:01:25.799447Z",
     "iopub.status.busy": "2022-05-01T08:01:25.799106Z",
     "iopub.status.idle": "2022-05-01T08:01:26.081078Z",
     "shell.execute_reply": "2022-05-01T08:01:26.08032Z",
     "shell.execute_reply.started": "2022-05-01T08:01:25.799396Z"
    }
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(discriminator, dpi=100, show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T08:01:26.083048Z",
     "iopub.status.busy": "2022-05-01T08:01:26.082574Z",
     "iopub.status.idle": "2022-05-01T08:01:26.093282Z",
     "shell.execute_reply": "2022-05-01T08:01:26.092457Z",
     "shell.execute_reply.started": "2022-05-01T08:01:26.083008Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"epochs\"):\n",
    "    os.mkdir(\"epochs\")\n",
    "def plot_examples(images_at_scales, epoch):\n",
    "    n_examples = images_at_scales[0].shape[0]\n",
    "    fig, axes = plt.subplots(figsize=(5, n_examples), nrows=n_examples, ncols=5, dpi=100)\n",
    "    for i in range(n_examples):\n",
    "        for j in range(5):\n",
    "            generated_images = images_at_scales[4-j]\n",
    "            gen_1 = generated_images[i].numpy()*127.5+127.5\n",
    "            axes[i, j].axis('off')\n",
    "            axes[i, j].imshow((gen_1).astype(\"uint32\"), interpolation='none')\n",
    "    plt.savefig('epochs/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T08:01:26.095389Z",
     "iopub.status.busy": "2022-05-01T08:01:26.095117Z",
     "iopub.status.idle": "2022-05-01T08:01:26.10424Z",
     "shell.execute_reply": "2022-05-01T08:01:26.10338Z",
     "shell.execute_reply.started": "2022-05-01T08:01:26.095353Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    \n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T08:01:26.106399Z",
     "iopub.status.busy": "2022-05-01T08:01:26.105847Z",
     "iopub.status.idle": "2022-05-01T08:01:26.113709Z",
     "shell.execute_reply": "2022-05-01T08:01:26.113006Z",
     "shell.execute_reply.started": "2022-05-01T08:01:26.106359Z"
    }
   },
   "outputs": [],
   "source": [
    "generator_optimizer = keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T08:01:26.117681Z",
     "iopub.status.busy": "2022-05-01T08:01:26.114785Z",
     "iopub.status.idle": "2022-05-01T08:01:26.124535Z",
     "shell.execute_reply": "2022-05-01T08:01:26.123767Z",
     "shell.execute_reply.started": "2022-05-01T08:01:26.117641Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T08:01:26.126196Z",
     "iopub.status.busy": "2022-05-01T08:01:26.125928Z",
     "iopub.status.idle": "2022-05-01T08:01:26.135652Z",
     "shell.execute_reply": "2022-05-01T08:01:26.134772Z",
     "shell.execute_reply.started": "2022-05-01T08:01:26.12616Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([batch_size, latent_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(\n",
    "        gen_loss,\n",
    "        generator.trainable_variables\n",
    "    )\n",
    "\n",
    "    gradients_of_discriminator = disc_tape.gradient(\n",
    "        disc_loss,\n",
    "        discriminator.trainable_variables\n",
    "    )\n",
    "\n",
    "    generator_optimizer.apply_gradients(\n",
    "        zip(gradients_of_generator,\n",
    "            generator.trainable_variables)\n",
    "        )\n",
    "    discriminator_optimizer.apply_gradients(\n",
    "        zip(reversed(gradients_of_discriminator),\n",
    "            reversed(discriminator.trainable_variables))\n",
    "        )\n",
    "\n",
    "    return (gen_loss, disc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T08:01:26.137633Z",
     "iopub.status.busy": "2022-05-01T08:01:26.136979Z",
     "iopub.status.idle": "2022-05-01T08:01:26.147454Z",
     "shell.execute_reply": "2022-05-01T08:01:26.146629Z",
     "shell.execute_reply.started": "2022-05-01T08:01:26.137441Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale_input_images(image_batch):\n",
    "    result = []\n",
    "    result.append(image_batch)\n",
    "    result.append(tf.image.resize(image_batch, [32, 32]))\n",
    "    result.append(tf.image.resize(image_batch, [16, 16]))\n",
    "    result.append(tf.image.resize(image_batch, [8, 8]))\n",
    "    result.append(tf.image.resize(image_batch, [4, 4]))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T08:01:26.150511Z",
     "iopub.status.busy": "2022-05-01T08:01:26.149784Z",
     "iopub.status.idle": "2022-05-01T08:01:26.163801Z",
     "shell.execute_reply": "2022-05-01T08:01:26.163023Z",
     "shell.execute_reply.started": "2022-05-01T08:01:26.150473Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    generator_losses = np.empty((0, 0), dtype=float)\n",
    "    discriminator_losses = np.empty((0, 0), dtype=float)\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        batch_generator_losses = np.empty((0, 0), dtype=float)\n",
    "        batch_discriminator_losses = np.empty((0, 0), dtype=float)\n",
    "        for (batch, image_batch) in enumerate(dataset):\n",
    "            scaled_batch = scale_input_images(image_batch)\n",
    "            gen_loss, disc_loss = train_step(scaled_batch)\n",
    "            \n",
    "            if batch % 100 == 0:\n",
    "                average_batch_loss =\\\n",
    "                   gen_loss.numpy()/int(image_batch.shape[1])\n",
    "                print(f\"\"\"Epoch {epoch+1}\n",
    "                        Batch {batch} Loss {average_batch_loss:.4f}\"\"\")\n",
    "\n",
    "            batch_generator_losses = np.append(batch_generator_losses, gen_loss)\n",
    "            batch_discriminator_losses = np.append(batch_discriminator_losses, disc_loss)\n",
    "            \n",
    "        if generator_losses.shape == (0, 0):\n",
    "            generator_losses = batch_generator_losses\n",
    "            discriminator_losses = batch_discriminator_losses\n",
    "        else:\n",
    "            generator_losses = np.vstack(\n",
    "                [generator_losses, batch_generator_losses]\n",
    "            )\n",
    "            discriminator_losses = np.vstack(\n",
    "                [discriminator_losses, batch_discriminator_losses]\n",
    "            )\n",
    "            \n",
    "        # Making a checkpoint every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            \n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "        example_images = generator(seed, training=False)\n",
    "        plot_examples(example_images, epoch)\n",
    "        \n",
    "\n",
    "    # Generating after the final epoch\n",
    "    example_images = generator(seed, training=False)\n",
    "    plot_examples(example_images, epoch)\n",
    "    \n",
    "    return (generator_losses, discriminator_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T08:01:26.167087Z",
     "iopub.status.busy": "2022-05-01T08:01:26.166616Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "(generator_losses, discriminator_losses) = train(dataset, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T10:24:42.5629Z",
     "iopub.status.busy": "2022-05-01T10:24:42.562635Z",
     "iopub.status.idle": "2022-05-01T10:24:43.086885Z",
     "shell.execute_reply": "2022-05-01T10:24:43.086267Z",
     "shell.execute_reply.started": "2022-05-01T10:24:42.56287Z"
    }
   },
   "outputs": [],
   "source": [
    "gen = generator_losses[:epochs].mean(axis=1) / batch_size\n",
    "disc = discriminator_losses[:epochs].mean(axis=1) / batch_size\n",
    "\n",
    "fig_1 = plt.figure(figsize=(8, 4), dpi=300)\n",
    "ax = fig_1.add_axes([0, 0, 1, 1])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('MSGGAN hibaértékei az első 200 epoch alatt')\n",
    "ax.plot(np.linspace(1, epochs, epochs), gen, label='Generator')\n",
    "ax.plot(np.linspace(1, epochs, epochs), disc, label='Discriminator')\n",
    "ax.legend(loc=0)\n",
    "\n",
    "ax.grid(True, color='0.6', dashes=(5, 2, 1, 2))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(0.05))\n",
    "\n",
    "x_ticks = np.arange(0, epochs+1, 10)\n",
    "x_ticks[0] = 1\n",
    "ax.set_xticks(x_ticks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T10:22:14.51753Z",
     "iopub.status.busy": "2022-05-01T10:22:14.516977Z",
     "iopub.status.idle": "2022-05-01T10:22:17.286858Z",
     "shell.execute_reply": "2022-05-01T10:22:17.282807Z",
     "shell.execute_reply.started": "2022-05-01T10:22:14.517493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generating samples\n",
    "noises = tf.random.normal([16, latent_dim])\n",
    "example_images = generator(noises, training=False)\n",
    "\n",
    "plt.imshow((example_images[0][0].numpy()*127.5+127.5).astype(\"uint32\"), interpolation='none')\n",
    "plot_examples(example_images, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T10:21:33.898887Z",
     "iopub.status.busy": "2022-05-01T10:21:33.898359Z",
     "iopub.status.idle": "2022-05-01T10:21:34.033137Z",
     "shell.execute_reply": "2022-05-01T10:21:34.032426Z",
     "shell.execute_reply.started": "2022-05-01T10:21:33.898848Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the trained models\n",
    "generator.save(\"msgGeneratornew.h5\")\n",
    "discriminator.save(\"msgDiscriminatornew.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-01T10:22:29.894753Z",
     "iopub.status.busy": "2022-05-01T10:22:29.893853Z",
     "iopub.status.idle": "2022-05-01T10:22:29.909665Z",
     "shell.execute_reply": "2022-05-01T10:22:29.908761Z",
     "shell.execute_reply.started": "2022-05-01T10:22:29.894706Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save(\"msg_generator_losses.npy\", generator_losses)\n",
    "np.save(\"msg_discriminator_losses.npy\", discriminator_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
