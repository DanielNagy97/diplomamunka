{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-scale gradient / Unet GAN 8x8-as változata (egy kapcsolattal)\n",
    "\n",
    "// Ez csak egy jegyzet és kezdetleges verzió //\n",
    "A 4x4-es egy szintel való kibővítése,\n",
    "a generátor kigenerálja a 4x4-es és 8x8-as képeket, amelyeket átad a diszkriminátornak.\n",
    "A kapcsolat a 4x4-es képnél van (Concat művelet)\n",
    "\n",
    "a tanulás során a 8x8-as kimeneten hamarabb konvergál a megoldás felé a generátor\n",
    "Ezt megfigyeltem a nagyobb modellnél is, hogy a nagyobb felbontáson tanulja be a mintákat először, majd a kisseb felbontásokon...\n",
    "\n",
    "16x16-nál egyből mode collapse lép fel már így haladva...\n",
    "\n",
    "Lehetséges, hogy a loss function-ön kell valamit módosítani (ez nem volt teljesen egyértelmű az MSG cikkben)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-27T13:18:36.053194Z",
     "iopub.status.busy": "2022-02-27T13:18:36.052872Z",
     "iopub.status.idle": "2022-02-27T13:18:40.866822Z",
     "shell.execute_reply": "2022-02-27T13:18:40.866114Z",
     "shell.execute_reply.started": "2022-02-27T13:18:36.053110Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from tensorflow import random\n",
    "from tensorflow import GradientTape\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-27T13:18:40.868769Z",
     "iopub.status.busy": "2022-02-27T13:18:40.868514Z",
     "iopub.status.idle": "2022-02-27T13:18:54.310267Z",
     "shell.execute_reply": "2022-02-27T13:18:54.308749Z",
     "shell.execute_reply.started": "2022-02-27T13:18:40.868736Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "number_of_examples = 8\n",
    "batch_size = 16\n",
    "latent_dim = 512\n",
    "image_size = (8, 8) # h x w\n",
    "\n",
    "seed = tf.random.normal([number_of_examples, latent_dim])\n",
    "\n",
    "data_dir = r'../input/animal-faces/afhq/train/'\n",
    "\n",
    "dataset = image_dataset_from_directory(\n",
    "    data_dir, label_mode=None, image_size=image_size, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-27T13:18:54.311829Z",
     "iopub.status.busy": "2022-02-27T13:18:54.311582Z",
     "iopub.status.idle": "2022-02-27T13:18:54.335360Z",
     "shell.execute_reply": "2022-02-27T13:18:54.334709Z",
     "shell.execute_reply.started": "2022-02-27T13:18:54.311796Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: (x - 127.5) / 127.5) # Normalizing to -1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-27T13:18:54.338700Z",
     "iopub.status.busy": "2022-02-27T13:18:54.338509Z",
     "iopub.status.idle": "2022-02-27T13:18:54.347699Z",
     "shell.execute_reply": "2022-02-27T13:18:54.346990Z",
     "shell.execute_reply.started": "2022-02-27T13:18:54.338677Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_generator_model(latent_dim):\n",
    "    visible = Input(shape=[latent_dim])\n",
    "    hidden = Reshape((1, 1, latent_dim))(visible)\n",
    "\n",
    "    hidden = Conv2DTranspose(filters=512, kernel_size=4, strides=(1, 1),\n",
    "                             padding='valid', activation=\"leaky_relu\")(hidden)\n",
    "    hidden = Conv2D(filters=512, kernel_size=3, strides=(1, 1),\n",
    "                             padding='same', activation=\"leaky_relu\")(hidden)\n",
    "    \n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    out4x4 = hidden\n",
    "    out4x4 = Conv2D(filters=3, kernel_size=4, strides=(1, 1), padding='same')(out4x4)\n",
    "    out4x4 = Activation(\"tanh\")(out4x4)\n",
    "    \n",
    "    \n",
    "    hidden = UpSampling2D(interpolation=\"bilinear\")(hidden)\n",
    "\n",
    "    \n",
    "    hidden = Conv2D(filters=256, kernel_size=3, strides=(1, 1),\n",
    "                             padding='same', activation=\"leaky_relu\")(hidden)\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    hidden = Conv2D(filters=256, kernel_size=3, strides=(1, 1),\n",
    "                             padding='same', activation=\"leaky_relu\")(hidden)\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    out8x8 = hidden\n",
    "    out8x8 = Conv2D(filters=3, kernel_size=4, strides=(1, 1), padding='same')(out8x8)\n",
    "    out8x8 = Activation(\"tanh\")(out8x8)\n",
    "    \n",
    "\n",
    "    model = Model(inputs=visible, outputs=[out8x8, out4x4])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-27T13:18:54.350445Z",
     "iopub.status.busy": "2022-02-27T13:18:54.348692Z",
     "iopub.status.idle": "2022-02-27T13:19:00.273847Z",
     "shell.execute_reply": "2022-02-27T13:19:00.273142Z",
     "shell.execute_reply.started": "2022-02-27T13:18:54.350402Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = make_generator_model(latent_dim)\n",
    "\n",
    "noise = tf.random.normal([1, latent_dim])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "print(generated_image[0].shape)\n",
    "\n",
    "plt.imshow((generated_image[0][0].numpy()*127.5+127.5).astype(\"uint32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-27T13:19:00.275237Z",
     "iopub.status.busy": "2022-02-27T13:19:00.274995Z",
     "iopub.status.idle": "2022-02-27T13:19:00.288804Z",
     "shell.execute_reply": "2022-02-27T13:19:00.288129Z",
     "shell.execute_reply.started": "2022-02-27T13:19:00.275202Z"
    }
   },
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-27T13:19:00.289848Z",
     "iopub.status.busy": "2022-02-27T13:19:00.289673Z",
     "iopub.status.idle": "2022-02-27T13:19:00.299673Z",
     "shell.execute_reply": "2022-02-27T13:19:00.298992Z",
     "shell.execute_reply.started": "2022-02-27T13:19:00.289827Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    input4 = Input(shape=(8, 8, 3))\n",
    "    \n",
    "    hidden = Conv2D(filters=256, kernel_size=4, strides=1, padding=\"same\",\n",
    "                    activation=\"tanh\")(input4) # From RGB\n",
    "\n",
    "    hidden = Conv2D(filters=256, kernel_size=3, strides=1, padding=\"same\",\n",
    "                    activation=\"leaky_relu\")(hidden)\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    hidden = Conv2D(filters=512, kernel_size=3, strides=1, padding=\"same\",\n",
    "                    activation=\"leaky_relu\")(hidden)\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    hidden = AveragePooling2D()(hidden)\n",
    "    \n",
    "    \n",
    "    input5 = Input(shape=(4, 4, 3))\n",
    "    hidden = Concatenate()([input5, hidden])\n",
    "    \n",
    "    hidden = Conv2D(filters=512, kernel_size=3, strides=1, padding=\"same\",\n",
    "                    activation=\"leaky_relu\")(hidden)\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    \n",
    "    \n",
    "    hidden = Conv2D(filters=512, kernel_size=4, strides=1, padding=\"valid\", activation=\"leaky_relu\")(hidden)\n",
    "    hidden = Flatten()(hidden)\n",
    "    hidden = Dense(1)(hidden)\n",
    "    out = Activation(\"sigmoid\")(hidden)\n",
    "\n",
    "    model = Model(inputs=[input4, input5], outputs=[out])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-27T13:19:00.301403Z",
     "iopub.status.busy": "2022-02-27T13:19:00.300879Z",
     "iopub.status.idle": "2022-02-27T13:19:00.391422Z",
     "shell.execute_reply": "2022-02-27T13:19:00.390799Z",
     "shell.execute_reply.started": "2022-02-27T13:19:00.301366Z"
    }
   },
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-27T13:19:00.394175Z",
     "iopub.status.busy": "2022-02-27T13:19:00.393984Z",
     "iopub.status.idle": "2022-02-27T13:19:01.227046Z",
     "shell.execute_reply": "2022-02-27T13:19:01.226150Z",
     "shell.execute_reply.started": "2022-02-27T13:19:00.394153Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-27T13:19:01.231659Z",
     "iopub.status.busy": "2022-02-27T13:19:01.230813Z",
     "iopub.status.idle": "2022-02-27T13:19:01.289082Z",
     "shell.execute_reply": "2022-02-27T13:19:01.288259Z",
     "shell.execute_reply.started": "2022-02-27T13:19:01.231593Z"
    }
   },
   "outputs": [],
   "source": [
    "decision = discriminator(generated_image, training=False)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-27T13:19:01.290669Z",
     "iopub.status.busy": "2022-02-27T13:19:01.290322Z",
     "iopub.status.idle": "2022-02-27T13:19:01.299943Z",
     "shell.execute_reply": "2022-02-27T13:19:01.298965Z",
     "shell.execute_reply.started": "2022-02-27T13:19:01.290632Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"epochs\"):\n",
    "    os.mkdir(\"epochs\")\n",
    "def plot_examples(images_at_scales, epoch):\n",
    "    examples = images_at_scales[0].shape[0]\n",
    "    output_lenght = len(images_at_scales)\n",
    "\n",
    "    fig, axes = plt.subplots(figsize=(output_lenght, examples),\n",
    "                             nrows=examples, ncols=output_lenght, dpi=200)\n",
    "\n",
    "    for i in range(examples):\n",
    "        for j in range(output_lenght):\n",
    "            generated_images = images_at_scales[output_lenght-1-j]\n",
    "            gen_1 = generated_images[i].numpy()*127.5+127.5\n",
    "            axes[i, j].axis('off')\n",
    "            axes[i, j].imshow((gen_1).astype(\"uint32\"), interpolation='none')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-27T13:19:01.301677Z",
     "iopub.status.busy": "2022-02-27T13:19:01.301394Z",
     "iopub.status.idle": "2022-02-27T13:19:01.308224Z",
     "shell.execute_reply": "2022-02-27T13:19:01.307518Z",
     "shell.execute_reply.started": "2022-02-27T13:19:01.301641Z"
    }
   },
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    \n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "# The generator is performing well, if the discriminator classifies fakes as real(1)\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-27T13:19:01.310016Z",
     "iopub.status.busy": "2022-02-27T13:19:01.309493Z",
     "iopub.status.idle": "2022-02-27T13:19:01.317916Z",
     "shell.execute_reply": "2022-02-27T13:19:01.317135Z",
     "shell.execute_reply.started": "2022-02-27T13:19:01.309865Z"
    }
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "#generator_optimizer = tf.keras.optimizers.RMSprop(1e-4)\n",
    "\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "#discriminator_optimizer = tf.keras.optimizers.RMSprop(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-27T13:19:01.319827Z",
     "iopub.status.busy": "2022-02-27T13:19:01.319361Z",
     "iopub.status.idle": "2022-02-27T13:19:01.327005Z",
     "shell.execute_reply": "2022-02-27T13:19:01.326190Z",
     "shell.execute_reply.started": "2022-02-27T13:19:01.319792Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-27T13:19:01.329245Z",
     "iopub.status.busy": "2022-02-27T13:19:01.329013Z",
     "iopub.status.idle": "2022-02-27T13:19:01.338130Z",
     "shell.execute_reply": "2022-02-27T13:19:01.337358Z",
     "shell.execute_reply.started": "2022-02-27T13:19:01.329221Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = random.normal([batch_size, latent_dim])\n",
    "\n",
    "    with GradientTape() as gen_tape, GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(\n",
    "        gen_loss,\n",
    "        generator.trainable_variables\n",
    "    )\n",
    "\n",
    "    gradients_of_discriminator = disc_tape.gradient(\n",
    "        disc_loss,\n",
    "        discriminator.trainable_variables\n",
    "    )\n",
    "\n",
    "    generator_optimizer.apply_gradients(\n",
    "        zip(gradients_of_generator,\n",
    "            generator.trainable_variables)\n",
    "        )\n",
    "    discriminator_optimizer.apply_gradients(\n",
    "        zip(gradients_of_discriminator,\n",
    "            discriminator.trainable_variables)\n",
    "        )\n",
    "\n",
    "    return (gen_loss, disc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-27T13:19:01.339933Z",
     "iopub.status.busy": "2022-02-27T13:19:01.339341Z",
     "iopub.status.idle": "2022-02-27T13:19:01.348054Z",
     "shell.execute_reply": "2022-02-27T13:19:01.347366Z",
     "shell.execute_reply.started": "2022-02-27T13:19:01.339888Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale_input_images(image_batch):\n",
    "    result = []\n",
    "    result.append(image_batch)\n",
    "    #result.append(tf.image.resize(image_batch, [32, 32]))\n",
    "    #result.append(tf.image.resize(image_batch, [16, 16]))\n",
    "    #result.append(tf.image.resize(image_batch, [8, 8]))\n",
    "    result.append(tf.image.resize(image_batch, [4, 4]))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-27T13:19:01.350114Z",
     "iopub.status.busy": "2022-02-27T13:19:01.349581Z",
     "iopub.status.idle": "2022-02-27T13:19:01.362457Z",
     "shell.execute_reply": "2022-02-27T13:19:01.361682Z",
     "shell.execute_reply.started": "2022-02-27T13:19:01.350079Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    generator_losses = np.empty((0, 0), dtype=float)\n",
    "    discriminator_losses = np.empty((0, 0), dtype=float)\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        batch_generator_losses = np.empty((0, 0), dtype=float)\n",
    "        batch_discriminator_losses = np.empty((0, 0), dtype=float)\n",
    "        for (batch, image_batch) in enumerate(dataset):\n",
    "            scaled_batch = scale_input_images(image_batch)\n",
    "            gen_loss, disc_loss = train_step(scaled_batch)\n",
    "            \n",
    "            if batch % 100 == 0:\n",
    "                average_batch_loss =\\\n",
    "                   gen_loss.numpy()/int(image_batch.shape[1])\n",
    "                print(f\"\"\"Epoch {epoch+1}\n",
    "                        Batch {batch} Loss {average_batch_loss:.4f}\"\"\")\n",
    "\n",
    "            batch_generator_losses = np.append(batch_generator_losses, gen_loss)\n",
    "            batch_discriminator_losses = np.append(batch_discriminator_losses, disc_loss)\n",
    "            \n",
    "        if generator_losses.shape == (0, 0):\n",
    "            generator_losses = batch_generator_losses\n",
    "            discriminator_losses = batch_discriminator_losses\n",
    "        else:\n",
    "            generator_losses = np.vstack(\n",
    "                [generator_losses, batch_generator_losses]\n",
    "            )\n",
    "            discriminator_losses = np.vstack(\n",
    "                [discriminator_losses, batch_discriminator_losses]\n",
    "            )\n",
    "            \n",
    "        # Saving the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            \n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "        # Producing images for the GIF\n",
    "        #display.clear_output(wait=True)\n",
    "        example_images = generator(seed, training=False)\n",
    "        plot_examples(example_images, epoch)\n",
    "        \n",
    "\n",
    "    # Generating after the final epoch\n",
    "    example_images = generator(seed, training=False)\n",
    "    plot_examples(example_images, epoch)\n",
    "    \n",
    "    return (generator_losses, discriminator_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-27T13:19:01.364398Z",
     "iopub.status.busy": "2022-02-27T13:19:01.363886Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "(generator_losses, discriminator_losses) = train(dataset, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
