{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T13:23:31.267984Z",
     "iopub.status.busy": "2022-02-25T13:23:31.267730Z",
     "iopub.status.idle": "2022-02-25T13:23:31.276266Z",
     "shell.execute_reply": "2022-02-25T13:23:31.275469Z",
     "shell.execute_reply.started": "2022-02-25T13:23:31.267955Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-28 18:11:22.933424: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-28 18:11:22.933457: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T13:23:31.710591Z",
     "iopub.status.busy": "2022-02-25T13:23:31.710324Z",
     "iopub.status.idle": "2022-02-25T13:23:41.554900Z",
     "shell.execute_reply": "2022-02-25T13:23:41.553350Z",
     "shell.execute_reply.started": "2022-02-25T13:23:31.710557Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-28 18:11:25.911828: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-02-28 18:11:25.911880: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-02-28 18:11:25.911918: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Inspiron-5558): /proc/driver/nvidia/version does not exist\n",
      "2022-02-28 18:11:25.912359: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 files belonging to 1 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No images found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22637/2075732561.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'../input/animal-faces/afhq/train/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m dataset = image_dataset_from_directory(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/preprocessing/image_dataset.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m       image_paths, labels, validation_split, subset)\n\u001b[1;32m    206\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No images found.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m   dataset = paths_and_labels_to_dataset(\n",
      "\u001b[0;31mValueError\u001b[0m: No images found."
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "number_of_examples = 8\n",
    "batch_size = 16\n",
    "latent_dim = 100\n",
    "image_size = (64, 64) # h x w\n",
    "\n",
    "seed = tf.random.normal([number_of_examples, latent_dim])\n",
    "\n",
    "data_dir = r'../input/animal-faces/afhq/train/'\n",
    "\n",
    "dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir, label_mode=None, image_size=image_size, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T13:23:47.016236Z",
     "iopub.status.busy": "2022-02-25T13:23:47.015972Z",
     "iopub.status.idle": "2022-02-25T13:23:47.040403Z",
     "shell.execute_reply": "2022-02-25T13:23:47.039771Z",
     "shell.execute_reply.started": "2022-02-25T13:23:47.016207Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: (x - 127.5) / 127.5) # Normalizing to -1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T13:37:21.218353Z",
     "iopub.status.busy": "2022-02-25T13:37:21.218094Z",
     "iopub.status.idle": "2022-02-25T13:37:21.230757Z",
     "shell.execute_reply": "2022-02-25T13:37:21.230077Z",
     "shell.execute_reply.started": "2022-02-25T13:37:21.218323Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_generator_model(latent_dim):\n",
    "    visible = keras.layers.Input(shape=[latent_dim])\n",
    "    hidden = keras.layers.Reshape((1, 1, 100))(visible)\n",
    "\n",
    "    hidden = keras.layers.Conv2DTranspose(filters=512, kernel_size=4, strides=(1, 1),\n",
    "                                          padding='valid', activation=\"relu\")(hidden)\n",
    "    hidden = keras.layers.BatchNormalization()(hidden)\n",
    "    out4x4 = hidden\n",
    "    out4x4 = keras.layers.Conv2D(filters=3, kernel_size=4, strides=(1, 1), padding='same')(out4x4)\n",
    "    out4x4 = keras.layers.Activation(\"tanh\")(out4x4)\n",
    "\n",
    "\n",
    "    hidden = keras.layers.Conv2DTranspose(filters=256, kernel_size=4, strides=(2, 2),\n",
    "                                          padding='same', activation=\"relu\")(hidden)\n",
    "    hidden = keras.layers.BatchNormalization()(hidden)\n",
    "    out8x8 = hidden\n",
    "    out8x8 = keras.layers.Conv2D(filters=3, kernel_size=4, strides=(1, 1), padding='same')(out8x8)\n",
    "    out8x8 = keras.layers.Activation(\"tanh\")(out8x8)\n",
    "\n",
    "\n",
    "    hidden = keras.layers.Conv2DTranspose(filters=128, kernel_size=4, strides=(2, 2),\n",
    "                                          padding='same', activation=\"relu\")(hidden)\n",
    "    hidden = keras.layers.BatchNormalization()(hidden)\n",
    "    out16x16 = hidden\n",
    "    out16x16 = keras.layers.Conv2D(filters=3, kernel_size=4, strides=(1, 1), padding='same')(out16x16)\n",
    "    out16x16 = keras.layers.Activation(\"tanh\")(out16x16)\n",
    "\n",
    "\n",
    "    hidden = keras.layers.Conv2DTranspose(filters=64, kernel_size=4, strides=(2, 2),\n",
    "                                          padding='same', activation=\"relu\")(hidden)\n",
    "    hidden = keras.layers.BatchNormalization()(hidden)\n",
    "    out32x32 = hidden\n",
    "    out32x32 = keras.layers.Conv2D(filters=3, kernel_size=4, strides=(1, 1), padding='same')(out32x32)\n",
    "    out32x32 = keras.layers.Activation(\"tanh\")(out32x32)\n",
    "\n",
    "\n",
    "    hidden = keras.layers.Conv2DTranspose(filters=64, kernel_size=4, strides=(2, 2),\n",
    "                                          padding='same', activation=\"relu\")(hidden)\n",
    "    hidden = keras.layers.BatchNormalization()(hidden)\n",
    "    hidden = keras.layers.Conv2D(filters=3, kernel_size=4, strides=(1, 1), padding='same')(hidden)\n",
    "    out64x64 = keras.layers.Activation(\"tanh\")(hidden)\n",
    "    model = keras.models.Model(inputs=visible, outputs=[out64x64, out32x32, out16x16, out8x8, out4x4])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T13:37:21.649787Z",
     "iopub.status.busy": "2022-02-25T13:37:21.648879Z",
     "iopub.status.idle": "2022-02-25T13:37:21.988625Z",
     "shell.execute_reply": "2022-02-25T13:37:21.987909Z",
     "shell.execute_reply.started": "2022-02-25T13:37:21.649738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff31ab41220>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAb7UlEQVR4nO2db+xlRXnHv98Fqf7UsqB2s7Ck0Eg0vKhgNojRGIViqDXCC0M0ptk0m+wb22BqotAmTUz6Qt/450XTZCPWfWEVitolxKh0hTRNGuRHQQVWBCnG3Sysbdlo+0tskacv7ll+M3PuPHfu3HPuvT/m+9mc7D1n5sw858/8zvPM88wMzQxCiJc/u1YtgBBiOaixC9EIauxCNIIauxCNoMYuRCOosQvRCAs1dpI3kHyC5FMkbx1KKCHE8LDWz07yHAA/AXA9gBMAHgTwYTN7fDjxhBBDce4C514N4CkzexoASH4NwI0Aso19Y2PDdu/evUCVQgiPM2fOYGtri9PSFmnsFwP4ebB/AsDbvBN2796NQ4cOLVClEMLj8OHD2bTRO+hIHiK5SXJza2tr7OqEEBkWaewnAVwS7O/rjkWY2WEz229m+zc2NhaoTgixCIs09gcBXE7yMpLnAfgQgLuHEUsIMTTVNruZvUDyTwF8B8A5AL5kZo8NJpkQYlAW6aCDmX0LwLcGkkUIMSILNfaVEIYFTHUwrDe++OGRF92cQsyLwmWFaAQ1diEaYeep8TuMNBiZrh4fqO5GJ9+K6F3MwGUyqaD4HljwK8449q3bSValvuxCNIIauxCNoMYuRCPsPJvdM4yWaUAFdVlSFzO/Z7N+dnpkRTvXWW3PB/lSe7v8dnDKrykHamdNd96rNXlMRejLLkQjqLEL0Qg7T433WJFOVegVmo818ekUV12a0bkfHMNEs8zvXuVzlFlS1xrq9/qyC9EIauxCNMKaqvGpDrT44pNZDWuMqLAhyot6qb0inXvlqpWl3dShIJZNKqZWBa81E7KukTnesQrPQjUjmgL6sgvRCGrsQjSCGrsQjbCmNnudjR6bO3EZzBlAA9jUg5Gx1/yq6mxNC84rLt8tL616AOOz4rTyLpgXk/1V+srCe5yXY9E7qi+7EI2gxi5EI6ypGp/HU2XGnrZgGK9IvpRlzldRVX5vxI9nCmx/R0pNhnIVPJ/TjcJzS68bMTOMd7DszEXfCX3ZhWgENXYhGkGNXYhG2HE2+yodJDU2mTs3fBp9OuLFDRIV7LneetdS6torKj65pzXW/CwWD8kuZVXv8MwvO8kvkTxN8tHg2IUk7yX5ZPf/BeOKKYRYlBI1/ssAbkiO3QrgmJldDuBYty+EWGNmNnYz+2cA/5UcvhHAke73EQA3zV2zJVuUxnirgMmW7i0sY2HdUwTxhJyfwns1zP2weAvLZ7wNw7aMpdL6tzRNzW3eeYUple/OmNR20O0xs1Pd72cB7BlIHiHESCzcG29m7t8ukodIbpLc3NraWrQ6IUQltb3xz5Hca2anSO4FcDqX0cwOAzgMABdddFFZF226DFAFPcugeODHALUF6nR177Dbpe/cn1xSb66GYCBIqd6dZCsWsdiNsfgEEv0SQu/HduqupDx/zdy8XNnLnGvQ0HKo/bLfDeBA9/sAgKPDiCOEGIsS19tXAfwrgDeRPEHyIIBPA7ie5JMA/qDbF0KsMTPVeDP7cCbpuoFlEUKMyNIj6M7aK8PYKY6tnA7Qqim+0O7qRXTRSSud3DHst+hd5rZCxtr+DYZKnWeTBiPKehOCOEVUhRvWvRU13QPp1BW1QxqHiapcDoqNF6IR1NiFaISlq/FZFaZQz4lW+nQGkriq0sjTo3lz4fmUzfnO0EXlWQmlN8FTwQepK3cSoodWvDrTXAOIppeaWj9uoOZqptMbHH3ZhWgENXYhGkGNXYhGWJ/JKyI7MZ8W2cDz2JC5NGcSRTfNEZjRzJGee9BxZRXeD0sutMoV1zMoA3dbzn5Pzyt2vXkxt8m12PSdareqkzH7aJeN1noTQiyKGrsQjbA6Nb54ad2ByPluakffOaPGLIjPSiPojPm0vBxeUsWIrJk4brlc+T3/V6B2e6sb5Uy0JK3gcF+oJLMFgvQtl3yp5sgYRxgOwIgmhL7sQjSCGrsQjbA6Nb5ycL+rmuaD69I+a1e0ecvoabqOSlg6DTIj9TN7yhSppquqbnSa47mIe77zdVnqkcjtzDMuqIJeT707KGm6GN4KwKn3o1guJy1X4tCTXOjLLkQjqLEL0Qhq7EI0wvpE0AUM4X2oLcMq/FU9D11VZJnjHXQj6NIIven5vP4Nz6BMnYpl+eK8ka2czhoxwKwivXswXQx3AB+dvomylHy9/fLLGNoLpy+7EI2gxi5EI6xnBJ0zAKVatckNTknqojMiIpp73lP73Akf8oNkSl1l0UCVXlIYuRYOaHHU/ZSy8T4zHkYu/C0tZFc+LSejpyJ7g6gqXx43AjBHrd00IvqyC9EIauxCNIIauxCNsJautyHWeutjU375kz3YlGWJp6X1y/BCaW3az+l5+8V1BP6rpM8h6hJw5p7PTio5ObFIjrpRdU7/hodld6L9/sQW059Ff7RgPtba68fJ0it+NXZ6SMnyT5eQvI/k4yQfI3lLd/xCkveSfLL7/4LxxRVC1FKixr8A4ONmdgWAawB8lOQVAG4FcMzMLgdwrNsXQqwpJWu9nQJwqvv9K5LHAVwM4EYA7+6yHQFwP4BPFtfsaUMDDPfpq88VdbnRbnlV3Z2/3pviLl9dr/a5y/BcQYUjEFNN1HND5ZRW1yPl5XbV4LIJJGLnlzMO0nGXDrKs2IqYq4OO5KUArgLwAIA93R8CAHgWwJ5hRRNCDElxYyf5GgBfB/AxM/tlmGaT3qqpf3pJHiK5SXJza2trIWGFEPUUNXaSr8CkoX/FzL7RHX6O5N4ufS+A09PONbPDZrbfzPZvbGwMIbMQooKS3ngCuB3AcTP7bJB0N4AD3e8DAI4uJIkFG5OtgrSI3ObVVSpGrbjpeeEtiHeGwJzNkdG2N1/g5LzMVitzVEZP/LJrqX7FgozF5yUZB3+cFZT42d8B4I8B/IjkI92xvwDwaQB3kjwI4GcAbh5FQiHEIJT0xv8L8n/ErhtWHCHEWKxPBF316KpCcj4kSywZt67C4WBuwFX+PNf9UyJSr/hal1HG5eUFndVO1uDdK8dNmRXEy+UuywUnzRmZl32eqWtvCAXeaySzUWy8EI2gxi5EI6yPGh8yiNru6ZWhepuq0uGAGSfKKppMrVxHtmgijrKosL7iGA7CcXTfqK68UP0xG5mlrbyIQucWOHeqPOSt8Bz/joapZe/HZLd0po9SScpK61/mYqaAvuxCNIIauxCNoMYuRCOsp83u4No08awUSWI4Mmr6RBaT/ULbzfI2da6ufonxXu7a+t6esO58ie586lkp0vKdfE5q3cQWhczh58t4It0+jLlEydRVy+D3KkBfdiEaQY1diEbYcWq8ixfUxumquxsQlRaSLl007ZRp5+XwtFEv2ssVKjQhCpVC5x7Et9QzQgrLT1Vubw66MEIvKiItI198XFzOIRhf21yDmQqDKnPXMm99i6AvuxCNoMYuRCOosQvRCC/ftd6SjLui8NPAfp9nNFhN5e7oKq+Isoz9PofwQPi3vMw27mV1LoWOHZovPy3EEyRMybtLy9dwc0aluX0kjjGeW2jO6ztYkdGuL7sQjaDGLkQjrKfrzVuSKczmFhKXkRt55alUxdqW4z7yy4hTLQ7x2v7ZK9+7C5kZJdy1ptK/+Rkfo5PLi2Z0AxstuxPjDb5zyrCgwtL3xRtl6J1XVrovx5iRh/qyC9EIauxCNMLq1PhKHaV0ijhL9MWa6so738sGYsxKzc6f0IsGLJxzzZPE6yHPTZxR6bkoDCxz55krrcuN8nN05IwF1aO+Iz24p70JU2rKmx992YVoBDV2IRpBjV2IRlhP11sthQZPqZ1YylABUZFJ6c1n6c5xXmh8Rufko8Iy83XMJDe+zHN1lvbHlEb8uUK55IV0q3bkiNyntfIvSMlab68k+X2SPyD5GMlPdccvI/kAyadI3kHyvPHEFEIsSoka/2sA15rZWwBcCeAGktcA+AyAz5nZGwE8D+DgaFIKIRZmZmO3Cf/d7b6i2wzAtQDu6o4fAXDTYFJZshWfx+3NKTMqurqu7Y2IN7PtrU+Y00mJCmS8ZTMmae51heckW3heeNy7HZZsYRGOuK6MmWtO68pd/tRIu0xd0Wq1aSnOe5W7NjNGW/aaezdoPErXZz+nW8H1NIB7AfwUwBkze6HLcgLAxaNIKIQYhKLGbma/MbMrAewDcDWAN5dWQPIQyU2Sm1tbW3VSCiEWZi7Xm5mdAXAfgLcD2E3ybG/+PgAnM+ccNrP9ZrZ/Y2NjEVmFEAtQ0hv/BpK7u9+vAnA9gOOYNPoPdtkOADg6mFSe4eXaoduJoQ1Gzy5y6urZhoWCxGZubKCF//olBmlDdCx4tqDfsRBJlb2nYVXMX7cvebEhPfWy6s3c5J56BeYu2oG0aIsTMcQFzE2Jn30vgCMkz8Hkj8OdZnYPyccBfI3kXwN4GMDtI8ophFiQmY3dzH4I4Kopx5/GxH4XQuwAdkQEXaQEORFXNcFjHq63yUn05iD3I9LKLiA6b4qGWIJ7r4IDkbnBXUm2vFqbjj7Ls53PCa5zEzzlmmU71Swp+G0QFBsvRCOosQvRCDtCjZ9fIexTo24Vq5W9fKGuni+EzpTZniCFs0z7hcS6ejaJkZodZ/TEKBarxvSa4gnIJFXq2eVPfmjVPRK39gXMoC+7EI2gxi5EI6ixC9EIO8JmL8ZZwqfc6rIgJZ/m2YKRZ8wpwo/IcirYFRqpThmOf43I36tc1f4c+0ySCqPNvGWXsmneM8vLmDwZRyivB6LsXrlmv9tH4oixIPqyC9EIauxCNMJ6ruLq5XWj2sKM+Qri4jy1zHE1eRFuQ/ikvAnf7MVsvkjzde/H9r45dUX3J7lo9z7mMrqJXkReKEahKj1XXV4kX6Fc04vr45p2uYRZhc5GX3YhGkGNXYhGUGMXohHWZ623cD9dMbjCVOmPugpHojk2mBO+mbNYe24nOn4WZ9RbqZcoqs9fAzlXejwyz7GBvdFrnnlpnojZ85z+B6cI31jOTZmRugqdujw3a9W7WVrGsL43fdmFaAQ1diEaYX0i6ArdJ262vJcIWfXWGUHluz7yE1RkI+2SzDUjw/q7pZFqSRkslNGdbCMqMCkiNHOmFp05sBjpyLycNTRXYOPAMq5qkgt92YVoBDV2IRphfdT4iDkGOmROK5301+9NnScMKsBbsbN4MEaYzZnkonTAReWl+ANhBjZXnOLjupJv1K7g/XA8Of4qq/m0weeZG9lMyKEvuxCNoMYuRCOosQvRCGtqs5cvs5M7rSayqVec55bz7HJ3ooKykVf5c/xTikfmza61yxfYw70Cy2a2KHWXTl1TampdiWFeHGpXOOMIipPKqkoLGeC51FD8Ze+WbX6Y5D3d/mUkHyD5FMk7SJ43sGxCiAGZR42/BZMFHc/yGQCfM7M3AngewMEhBRNCDEtRYye5D8AfAfhit08A1wK4q8tyBMBNQwllyVYM85uBL22ltfVXJp1eV7+EIDGVxRG46pqdEr0Co3zFMpbWXKeAhs/IXz7KedC9MjPPxS1zAOZ47iNKEVH6Zf88gE9gezza6wCcMbMXuv0TAC4eVjQhxJCUrM/+fgCnzeyhmgpIHiK5SXJza2urpgghxACU9Ma/A8AHSL4PwCsB/DaALwDYTfLc7uu+D8DJaSeb2WEAhwHgoosuWkRDFUIswMwvu5ndZmb7zOxSAB8C8D0z+wiA+wB8sMt2AMDRoYSixVteOOTt0l5aeuDstrit2Ze3zvrO2tvOdbqX6VB0a2Y9B7Ngc+r2ZHK6N8rvYj5nJH/xY67uNSokX/6YtS4SVPNJAH9O8ilMbPjbhxFJCDEGcwXVmNn9AO7vfj8N4OrhRRJCjMF6zhvvjRRzR5TliUc1FQ7z6oXQZeKbBvOTlA7RymRLDpQG+ZWK5EadOetc+XPsM8iXuD69paGm1uS/OuXhad7dWlzBTpfqtuDaxnC5nUWx8UI0ghq7EI2wpgNhUupU94ji8zw1LVdI2kXOaT8nJXiqZFbvntIFP+UUIFaZvUuOqnLvjU39CcBZairOHE1bneTy5sKLBuE4EroeG9c8DCvz3rEK1d15MOYKPB76sgvRCGrsQjSCGrsQjbA+yz8NTLHl7dqhjsEdnZd3Se1K0tJ5zePzylw8Q1h8pd7HcMdbrqo/X3veTs/hzkvv4bkHo74DJ1f03LNFuMTe3fS55+suZzEXoL7sQjSCGrsQjbAjXG9Z5cWJwnMWJnXdMXnlC7DAnUJn3rP+0IaMjF7OsgC6GW6iQoXRi2Z0cVT10OIJVFqmLqmwNNc1NrXaGRmT8p2zXNPOcVOWijWM1bqYAacvuxCNoMYuRCOosQvRCOtpszvusAjPReKFqearmmF35ez0ZNJAL/TSCaXN4rkHexcQlJ+VMD6ShrqmTrQiwbz+Ey/c2SkiJ6P7zKr7H/J4fQn5pDlezlGmquijL7sQjaDGLkQjrKcaP0AklRfRFU2sMIhTJO9e6+FMVPBizsVTOIcG4LmJXIcgsomhvG60Yb6M0tWZUldkeb6yKL9hKLQTeqPevDKWg77sQjSCGrsQjbCearxDecSSo8+FPdbeuJR5BAuIJmtwVhx1OtL9gTxe73A2bY7Qr5xmOoeKXOxpyDyXfiGlPf+1Pd2lg0wKL2zkgV416MsuRCOosQvRCGrsQjTCymz22kCn4gFPhYFf1bFM7qi0jN9pDiqD6/JuOWdiynKZyu9WtnjvwfeKy5TvTgzhDHf06hp7EshhZq9YiKLGTvIZAL8C8BsAL5jZfpIXArgDwKUAngFws5k9P46YQohFmUeNf4+ZXWlm+7v9WwEcM7PLARzr9oUQa8oiNvuNAI50v48AuGmek4tXak2xYJsH4/YWSxJt4V6aNoyMXnnz19W7j1ERwU5yKWZ8aSvHkL+4/L3KypfK2Ksu88zC4zbjmYW7oeipHLXvVe4WpLfKS1sSpY3dAHyX5EMkD3XH9pjZqe73swD2DC6dEGIwSjvo3mlmJ0n+DoB7Sf44TDQzY39JEABA98fhEACcf/75CwkrhKin6MtuZie7/08D+CYmSzU/R3IvAHT/n86ce9jM9pvZ/o2NjWGkFkLMzczGTvLVJF979jeA9wJ4FMDdAA502Q4AODqWkLFAyJp4PttGUmwuJQZUlGjOeXUyWvDPkzE6Sos2z+Az2968fKS9tNUSl/5isoXyZ7pL0lJ6l1b4zKLSXoy2iKjzYA7D2cs2raipfROhHE7aiJSo8XsAfLOb9P5cAH9vZt8m+SCAO0keBPAzADePJ6YQYlFmNnYzexrAW6Yc/08A140hlBBieF62yz959blVeyPKKqotjnBz8CbYsCSarHQCiCGIak509HjCCid0zb0fDH5ZNl9chHdTt9N6z6X2fVzD0W05FBsvRCOosQvRCGrsQjTCjpupJmJaxOaYxVfZ27MOLMYybXTvdpcu5zzf/Si7Nr+I0NYP+wDmYAfZ5R76sgvRCGrsQjTCzlPjS1VpTwOM9PFd2bT+0kqFcrhpoU9qHjfUoqT+qrwrK3eh3v1gT493VPdlkpvFc55n5s+wsWPQl12IRlBjF6IRdp4aX6oTujq4F9FVqEuXBWrN6NJPqs5F+fWWEhogls81gYIBJMwJNaP8LJ45UeZe8Zb2cksofWZTaixKqrRXRrXeAvRlF6IR1NiFaAQ1diEaYcfZ7OX2TZ31YwNYTYx9UnOcl0tYXCbXE1lcn+OvKhbR6zsoK2Qur1llzuLTKh7NyIGfWfRlF6IR1NiFaISdocZPH8swCkMU7w78WBHDyJEqoMtyGqUkE3YUi7EeT2NVUujLLkQjqLEL0Qhq7EI0ws6w2QuNHNd0yyZ6jpy6ytbDMsQSTOrMiLK0vsHlyI+wG4W1eaCLoS+7EI2gxi5EI+wMNb6QKq9Lb0TZEJU51Ki0tZPhjax+RlJUjyhbtGZRStGXneRukneR/DHJ4yTfTvJCkveSfLL7/4KxhRVC1FOqxn8BwLfN7M2YLAV1HMCtAI6Z2eUAjnX7Qog1pWQV1/MBvAvA7QBgZv9rZmcA3AjgSJftCICbxhGxjuI1OmtX1CxcANStr+Yc1hYyPENLMce6qqKCki/7ZQB+AeDvSD5M8ovd0s17zOxUl+dZTFZ7FUKsKSWN/VwAbwXwt2Z2FYD/QaKym1n2jzHJQyQ3SW5ubW0tKq8QopKSxn4CwAkze6DbvwuTxv8cyb0A0P1/etrJZnbYzPab2f6NjY0hZBZCVDCzsZvZswB+TvJN3aHrADwO4G4AB7pjBwAcHUXCSmpN8aoKxCAw+SeGpdTP/mcAvkLyPABPA/gTTP5Q3EnyIICfAbh5HBGFEENQ1NjN7BEA+6ckXTeoNEKI0XhZRdCJnY4cbmOi2HghGkGNXYhGUGMXohHU2IVoBDV2IRpBjV2IRqClkzeMWRn5C0wCcF4P4D+WVvF01kEGQHKkSI6YeeX4XTN7w7SEpTb2lyolN81sWpBOUzJIDsmxTDmkxgvRCGrsQjTCqhr74RXVG7IOMgCSI0VyxAwmx0psdiHE8pEaL0QjLLWxk7yB5BMknyK5tNloSX6J5GmSjwbHlj4VNslLSN5H8nGSj5G8ZRWykHwlye+T/EEnx6e645eRfKB7Pnd08xeMDslzuvkN71mVHCSfIfkjko+Q3OyOreIdGW3a9qU1dpLnAPgbAH8I4AoAHyZ5xZKq/zKAG5Jjq5gK+wUAHzezKwBcA+Cj3T1Ytiy/BnCtmb0FwJUAbiB5DYDPAPicmb0RwPMADo4sx1luwWR68rOsSo73mNmVgatrFe/IeNO2m9lSNgBvB/CdYP82ALctsf5LATwa7D8BYG/3ey+AJ5YlSyDDUQDXr1IWABsA/g3A2zAJ3jh32vMasf593Qt8LYB7MJnoaxVyPAPg9cmxpT4XAOcD+Hd0fWlDy7FMNf5iAD8P9k90x1bFSqfCJnkpgKsAPLAKWTrV+RFMJgq9F8BPAZwxsxe6LMt6Pp8H8AkAL3b7r1uRHAbguyQfInmoO7bs5zLqtO3qoIM/FfYYkHwNgK8D+JiZ/XIVspjZb8zsSky+rFcDePPYdaaQfD+A02b20LLrnsI7zeytmJiZHyX5rjBxSc9loWnbZ7HMxn4SwCXB/r7u2Koomgp7aEi+ApOG/hUz+8YqZQEAm6zucx8m6vJukmenKlvG83kHgA+QfAbA1zBR5b+wAjlgZie7/08D+CYmfwCX/VwWmrZ9Fsts7A8CuLzraT0PwIcwmY56VSx9KmySxGQZreNm9tlVyULyDSR3d79fhUm/wXFMGv0HlyWHmd1mZvvM7FJM3ofvmdlHli0HyVeTfO3Z3wDeC+BRLPm52NjTto/d8ZF0NLwPwE8wsQ//con1fhXAKQD/h8lfz4OY2IbHADwJ4J8AXLgEOd6JiQr2QwCPdNv7li0LgN8H8HAnx6MA/qo7/nsAvg/gKQD/AOC3lviM3g3gnlXI0dX3g2577Oy7uaJ35EoAm92z+UcAFwwlhyLohGgEddAJ0Qhq7EI0ghq7EI2gxi5EI6ixC9EIauxCNIIauxCNoMYuRCP8P94vAO3CwEzwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model(latent_dim)\n",
    "\n",
    "noise = tf.random.normal([1, latent_dim])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow((generated_image[0][0].numpy()*127.5+127.5).astype(\"uint32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1, 1, 100)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 4, 4, 512)    819712      reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 4, 4, 512)    2048        conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 8, 8, 256)    2097408     batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 8, 8, 256)    1024        conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 16, 16, 128)  524416      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 128)  512         conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 32, 32, 64)   131136      batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 64)   256         conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 64, 64, 64)   65600       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64, 64, 64)   256         conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 64, 64, 3)    3075        batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 3)    3075        batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 3)    6147        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 8, 8, 3)      12291       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 4, 4, 3)      24579       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 64, 64, 3)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 3)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 16, 16, 3)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 8, 8, 3)      0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 4, 4, 3)      0           conv2d[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,691,535\n",
      "Trainable params: 3,689,487\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T13:37:22.671047Z",
     "iopub.status.busy": "2022-02-25T13:37:22.670397Z",
     "iopub.status.idle": "2022-02-25T13:37:22.683474Z",
     "shell.execute_reply": "2022-02-25T13:37:22.682601Z",
     "shell.execute_reply.started": "2022-02-25T13:37:22.671007Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    input1 = keras.layers.Input(shape=(64, 64, 3))\n",
    "    hidden = keras.layers.Conv2D(filters=64, kernel_size=4, strides=2, padding=\"same\",\n",
    "                                 activation=\"relu\", kernel_initializer=\"he_normal\")(input1)\n",
    "    hidden = keras.layers.BatchNormalization()(hidden)\n",
    "\n",
    "    input2 = keras.layers.Input(shape=(32, 32, 3))\n",
    "    hidden = keras.layers.Concatenate()([hidden, input2])\n",
    "    hidden = keras.layers.Conv2D(filters=128, kernel_size=4, strides=2, padding=\"same\",\n",
    "                                 activation=\"relu\", kernel_initializer=\"he_normal\")(hidden)\n",
    "    hidden = keras.layers.BatchNormalization()(hidden)\n",
    "\n",
    "    input3 = keras.layers.Input(shape=(16, 16, 3))\n",
    "    hidden = keras.layers.Concatenate()([hidden, input3])\n",
    "    hidden = keras.layers.Conv2D(filters=256, kernel_size=4, strides=2, padding=\"same\",\n",
    "                                 activation=\"relu\", kernel_initializer=\"he_normal\")(hidden)\n",
    "    hidden = keras.layers.BatchNormalization()(hidden)\n",
    "\n",
    "    input4 = keras.layers.Input(shape=(8, 8, 3))\n",
    "    hidden = keras.layers.Concatenate()([hidden, input4])\n",
    "    hidden = keras.layers.Conv2D(filters=512, kernel_size=4, strides=2, padding=\"same\",\n",
    "                                 activation=\"relu\", kernel_initializer=\"he_normal\")(hidden)\n",
    "    hidden = keras.layers.BatchNormalization()(hidden)\n",
    "\n",
    "    input5 = keras.layers.Input(shape=(4, 4, 3))\n",
    "    hidden = keras.layers.Concatenate()([hidden, input5])\n",
    "\n",
    "    hidden = keras.layers.Conv2D(filters=100, kernel_size=4, strides=1, padding=\"valid\", activation=\"relu\")(hidden)\n",
    "    hidden = keras.layers.Flatten()(hidden)\n",
    "    out = keras.layers.Dense(1)(hidden)\n",
    "    #out = keras.layers.Activation(\"sigmoid\")(hidden)\n",
    "    \n",
    "\n",
    "    model = keras.models.Model(inputs=[input1, input2, input3, input4, input5], outputs=[out])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T13:37:22.999476Z",
     "iopub.status.busy": "2022-02-25T13:37:22.999238Z",
     "iopub.status.idle": "2022-02-25T13:37:23.095635Z",
     "shell.execute_reply": "2022-02-25T13:37:23.094846Z",
     "shell.execute_reply.started": "2022-02-25T13:37:22.999448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.49975345]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image, training=False)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 64)   3136        input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 64)   256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 32, 32, 67)   0           batch_normalization_13[0][0]     \n",
      "                                                                 input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 128)  137344      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, 16, 16, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 16, 16, 131)  0           batch_normalization_14[0][0]     \n",
      "                                                                 input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 256)    536832      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 256)    1024        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           [(None, 8, 8, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 8, 8, 259)    0           batch_normalization_15[0][0]     \n",
      "                                                                 input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 4, 4, 512)    2122240     concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 4, 4, 512)    2048        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           [(None, 4, 4, 3)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 4, 4, 515)    0           batch_normalization_16[0][0]     \n",
      "                                                                 input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 1, 1, 100)    824100      concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 100)          0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            101         flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 1)            0           dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 3,627,593\n",
      "Trainable params: 3,625,673\n",
      "Non-trainable params: 1,920\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T13:37:23.650345Z",
     "iopub.status.busy": "2022-02-25T13:37:23.649681Z",
     "iopub.status.idle": "2022-02-25T13:37:23.656724Z",
     "shell.execute_reply": "2022-02-25T13:37:23.656033Z",
     "shell.execute_reply.started": "2022-02-25T13:37:23.650311Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"epochs\"):\n",
    "    os.mkdir(\"epochs\")\n",
    "def plot_examples(images_at_scales, epoch):\n",
    "    n_examples = images_at_scales[0].shape[0]\n",
    "    fig, axes = plt.subplots(figsize=(5, n_examples), nrows=n_examples, ncols=5, dpi=100)\n",
    "    for i in range(n_examples):\n",
    "        for j in range(5):\n",
    "            generated_images = images_at_scales[4-j]\n",
    "            gen_1 = generated_images[i].numpy()*127.5+127.5\n",
    "            axes[i, j].axis('off')\n",
    "            axes[i, j].imshow((gen_1).astype(\"uint32\"), interpolation='none')\n",
    "    plt.savefig('epochs/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T13:37:24.067819Z",
     "iopub.status.busy": "2022-02-25T13:37:24.067601Z",
     "iopub.status.idle": "2022-02-25T13:37:24.073333Z",
     "shell.execute_reply": "2022-02-25T13:37:24.072409Z",
     "shell.execute_reply.started": "2022-02-25T13:37:24.067794Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    \n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "# The generator is performing well, if the discriminator classifies fakes as real(1)\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T13:37:24.469860Z",
     "iopub.status.busy": "2022-02-25T13:37:24.469280Z",
     "iopub.status.idle": "2022-02-25T13:37:24.475042Z",
     "shell.execute_reply": "2022-02-25T13:37:24.473898Z",
     "shell.execute_reply.started": "2022-02-25T13:37:24.469820Z"
    }
   },
   "outputs": [],
   "source": [
    "generator_optimizer = keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T13:37:24.845970Z",
     "iopub.status.busy": "2022-02-25T13:37:24.845757Z",
     "iopub.status.idle": "2022-02-25T13:37:24.856405Z",
     "shell.execute_reply": "2022-02-25T13:37:24.855711Z",
     "shell.execute_reply.started": "2022-02-25T13:37:24.845945Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T13:37:25.209037Z",
     "iopub.status.busy": "2022-02-25T13:37:25.208672Z",
     "iopub.status.idle": "2022-02-25T13:37:25.263178Z",
     "shell.execute_reply": "2022-02-25T13:37:25.262527Z",
     "shell.execute_reply.started": "2022-02-25T13:37:25.209007Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = random.normal([batch_size, latent_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(\n",
    "        gen_loss,\n",
    "        generator.trainable_variables\n",
    "    )\n",
    "\n",
    "    gradients_of_discriminator = disc_tape.gradient(\n",
    "        disc_loss,\n",
    "        discriminator.trainable_variables\n",
    "    )\n",
    "\n",
    "    generator_optimizer.apply_gradients(\n",
    "        zip(gradients_of_generator,\n",
    "            generator.trainable_variables)\n",
    "        )\n",
    "    discriminator_optimizer.apply_gradients(\n",
    "        zip(gradients_of_discriminator,\n",
    "            discriminator.trainable_variables)\n",
    "        )\n",
    "\n",
    "    return (gen_loss, disc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T13:37:25.608139Z",
     "iopub.status.busy": "2022-02-25T13:37:25.607586Z",
     "iopub.status.idle": "2022-02-25T13:37:25.612974Z",
     "shell.execute_reply": "2022-02-25T13:37:25.612328Z",
     "shell.execute_reply.started": "2022-02-25T13:37:25.608109Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale_input_images(image_batch):\n",
    "    result = []\n",
    "    result.append(image_batch)\n",
    "    result.append(tf.image.resize(image_batch, [32, 32]))\n",
    "    result.append(tf.image.resize(image_batch, [16, 16]))\n",
    "    result.append(tf.image.resize(image_batch, [8, 8]))\n",
    "    result.append(tf.image.resize(image_batch, [4, 4]))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T13:37:25.980432Z",
     "iopub.status.busy": "2022-02-25T13:37:25.979806Z",
     "iopub.status.idle": "2022-02-25T13:37:25.992211Z",
     "shell.execute_reply": "2022-02-25T13:37:25.991447Z",
     "shell.execute_reply.started": "2022-02-25T13:37:25.980394Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    generator_losses = np.empty((0, 0), dtype=float)\n",
    "    discriminator_losses = np.empty((0, 0), dtype=float)\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        \n",
    "        batch_generator_losses = np.empty((0, 0), dtype=float)\n",
    "        batch_discriminator_losses = np.empty((0, 0), dtype=float)\n",
    "        for (batch, image_batch) in enumerate(dataset):\n",
    "            scaled_batch = scale_input_images(image_batch)\n",
    "            gen_loss, disc_loss = train_step(scaled_batch)\n",
    "            \n",
    "            if batch % 100 == 0:\n",
    "                average_batch_loss =\\\n",
    "                   gen_loss.numpy()/int(image_batch.shape[1])\n",
    "                print(f\"\"\"Epoch {epoch+1}\n",
    "                        Batch {batch} Loss {average_batch_loss:.4f}\"\"\")\n",
    "\n",
    "            batch_generator_losses = np.append(batch_generator_losses, gen_loss)\n",
    "            batch_discriminator_losses = np.append(batch_discriminator_losses, disc_loss)\n",
    "            \n",
    "        if generator_losses.shape == (0, 0):\n",
    "            generator_losses = batch_generator_losses\n",
    "            discriminator_losses = batch_discriminator_losses\n",
    "        else:\n",
    "            generator_losses = np.vstack(\n",
    "                [generator_losses, batch_generator_losses]\n",
    "            )\n",
    "            discriminator_losses = np.vstack(\n",
    "                [discriminator_losses, batch_discriminator_losses]\n",
    "            )\n",
    "            \n",
    "        # Saving the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            \n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "        # Producing images for the GIF\n",
    "        #display.clear_output(wait=True)\n",
    "        example_images = generator(seed, training=False)\n",
    "        plot_examples(example_images, epoch)\n",
    "        \n",
    "\n",
    "    # Generating after the final epoch\n",
    "    example_images = generator(seed, training=False)\n",
    "    plot_examples(example_images, epoch)\n",
    "    \n",
    "    return (generator_losses, discriminator_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T14:35:36.923217Z",
     "iopub.status.busy": "2022-02-25T14:35:36.922957Z",
     "iopub.status.idle": "2022-02-25T15:06:14.426315Z",
     "shell.execute_reply": "2022-02-25T15:06:14.425421Z",
     "shell.execute_reply.started": "2022-02-25T14:35:36.923186Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "(generator_losses, discriminator_losses) = train(dataset, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T15:06:37.318067Z",
     "iopub.status.busy": "2022-02-25T15:06:37.317775Z",
     "iopub.status.idle": "2022-02-25T15:06:37.608139Z",
     "shell.execute_reply": "2022-02-25T15:06:37.607360Z",
     "shell.execute_reply.started": "2022-02-25T15:06:37.318026Z"
    }
   },
   "outputs": [],
   "source": [
    "gen = generator_losses[:epochs].mean(axis=1) / batch_size\n",
    "disc = discriminator_losses[:epochs].mean(axis=1) / batch_size\n",
    "\n",
    "fig_1 = plt.figure(figsize=(8, 4), dpi=100)\n",
    "ax = fig_1.add_axes([0, 0, 1, 1])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('DCGAN hibaértékei az első 50 epoch alatt')\n",
    "ax.plot(np.linspace(1, epochs, epochs), gen, label='Generator')\n",
    "ax.plot(np.linspace(1, epochs, epochs), disc, label='Discriminator')\n",
    "ax.legend(loc=0)\n",
    "\n",
    "ax.grid(True, color='0.6', dashes=(5, 2, 1, 2))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(0.05))\n",
    "\n",
    "x_ticks = np.arange(0, epochs+1, 5)\n",
    "x_ticks[0] = 1\n",
    "ax.set_xticks(x_ticks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T15:11:20.907500Z",
     "iopub.status.busy": "2022-02-25T15:11:20.907252Z",
     "iopub.status.idle": "2022-02-25T15:11:23.906973Z",
     "shell.execute_reply": "2022-02-25T15:11:23.906375Z",
     "shell.execute_reply.started": "2022-02-25T15:11:20.907471Z"
    }
   },
   "outputs": [],
   "source": [
    "noises = tf.random.normal([16, latent_dim])\n",
    "example_images = generator(noises, training=False)\n",
    "\n",
    "plt.imshow((example_images[0][0].numpy()*127.5+127.5).astype(\"uint32\"), interpolation='none')\n",
    "plot_examples(example_images, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
