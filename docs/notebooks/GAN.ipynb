{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "754cf5ab",
   "metadata": {},
   "source": [
    "# GAN háló felépítése, tanítása és egyebek\n",
    "\n",
    "Ezen notebook a GAN hálókról kíván egy kis összefoglalást nyújtani. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56858dd",
   "metadata": {},
   "source": [
    "## Áttekintés\n",
    "\n",
    "A Generative Adverserial Network (GAN), vagy magyarul: Generatív Kontradiktórius Hálózat egy olyan generatív modell, amely nem a megszokott statisztikai alapokon optimalizál, mint például az autoencoder vagy pixelRNN modellek, hanem játékelméleti megközelítést alkalmaz.\n",
    "A tanulás során két neurális hálózat versenyzik egymással: egy generátor, amelynek az a szerepe, hogy a tanítómintákhoz hasonló adatot generáljon a bemeneti zajból és egy diszkriminátor, amely a legegyszerűbb esetben egy bináris osztályozó, amely a generátor által generált adatot vizsgálja és eldönti, hogy az valódi vagy hamis. A tanítás során optimális esetben minden egyes epoch-kal egyre valósághűbb adatot állít elő a generátor és ezzel egyidejűleg is a diszkriminátor is fejlődni fog, tehát egyre pontosabban tudja megállapítani a valós és hamis adatokat és arra ösztönzi a generátort, hogy ő is minél pontosabban állítsa elő a hamis adatokat és így tovább..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c98ec",
   "metadata": {},
   "source": [
    "## Tanítási folyamat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf281c53",
   "metadata": {},
   "source": [
    "### Adathalmaz\n",
    "A tanításhoz szükségünk van egy megfelelő adathalmazra.\n",
    "Legegyszerűbb esetben elegendő lehet képek rendezetlen halmaza is és a modellre bízhatjuk, hogy ismerje fel az egyes osztályok jellegzetességeit. Megfelelő regularizációs technikákkal igazán változatosan ki lehet tölteni a látens teret. Viszont annak feltérképezése már nem evidens feladat. Hiszen képeket a látens tér egyes pontjainak kijelölésével és a ponthoz tartozó generátor kimenettel tudunk generálni. Vagyis a generátor segítségével mintavételezhetjük a látens teret. Viszont arról nem rendelkezünk előzetes információkkal, hogy a modell hogyan töltötte ki ezt a teret.\n",
    "\n",
    "Ha az adathalmaz rendelkezik osztályokkal is, úgy az osztály-címkéket is felhasználhatjuk a GAN hálózatunk tanításához. Így egy plusz bemenet segítségével könnyebben tudunk majd megfelelő képeket előállítani.\n",
    "\n",
    "Egyes adathalmazokhoz igen részletes annotációkat is mellékelnek. A képeken megfigyelhető objektumokat határoló dobozokkal, vagy pixel szinten jelölik. Így igen részletes információkat kínálnak a kép tartalmáról.\n",
    "Egy igazán hasznos annotációk lehetnek a természetes nyelvű leíró mondatok is, amelyekből általában többet is mellékelnek egy-egy képhez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb1fd8a",
   "metadata": {},
   "source": [
    "### Tanítás\n",
    "\n",
    "Ha a legegyszerűbb esetet vizsgáljuk és csak a képek rendezetlen halmazára tanítjuk a modellt, mindenféle kiegészítő információ és annotáció nélkül, akkor a tanítás a következőképpen zajlik:\n",
    "A tanítómintát érdemes úgy összeállítani, hogy feldarabolható legyen, ezzel is megkönnyítve a párhuzamos feldolgozást. Az ilyen modellek tanításához általában igen sok képre van szükség és a teljes tanítóminta beadása a modell tanítását ellehetetlenítheti. Ha egy lépésben láthatná a diszkriminátor az összes képet, akkor túlságosan is megnőne a teljesítménye és a generátornak esélye sem lenne felzárkózni. Ehelyett a tanítóminta darabjain célszerű tanítani a GAN hálózatot, úgynevezett minibatch-okon. Így az egyes darabok feldolgozása után történne meg a generátor és a diszkriminátor frissítése, lehetőséget adva a generátornak, hogy megfelelő ütemben tanuljon.\n",
    "Viszont ha minél változatosabb képeket generáló GAN hálózatot kívánunk létrehozni, úgy egyes cikkek szerint(melyek azok?) nagy batch mérettel kell tanítani a hálót.\n",
    "Egy Epoch alatt a tanítóhalmaz összes batch-én való tanítást értjük.\n",
    "\n",
    "A továbbiakban a következő jelöléseket használom: legyen $D$ a _Diszkriminátor_, $G$ pedig a _Generátor_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da84c29e",
   "metadata": {},
   "source": [
    "#### Hibafüggvények\n",
    "\n",
    "A _Generátor_ és _Diszkriminátor_ hibájának számolása a bináris kereszt-entrópián alapszik.\n",
    "\n",
    "A bináris kereszt-entrópia hibafüggvény a következőképpen írható fel:\n",
    "$$L(\\hat y, y) = y . \\log \\hat y + (1-y). \\log (1 - \\hat y)$$\n",
    "Ahol $\\hat y$ a predikció, $y$ pedig a valós címke\n",
    "\n",
    "##### Diszkriminátor hibafüggvénye\n",
    "A _Generátor_ egy z bemeneti zajvektor alapján előállít egy generált adatot $G(z)$\n",
    "\n",
    "A Diszkriminátor egy bináris osztályozó, amelynek feladata, hogy az $x$ és $G(z)$ bemeneteit osztályozza. $D(x)$ esetén 1, $D(G(z))$ esetén pedig 0 címkét várunk.\n",
    "\n",
    "Így a kereszt-entrópia hibafüggvény $D(x)$ esetében a következő:\n",
    "$$L(D(x), 1) = 1.\\log D(x) + (1 - 1).\\log(1 - D(x))$$\n",
    "$$L(D(x), 1) = \\log D(x)$$\n",
    "\n",
    "A _Diszkriminátornak_ a $\\log(D(x))$-et kell maximalizálnia.\n",
    "\n",
    "$D(G(z))$ esetében a kereszt-entrópia hibafüggvény a következő:\n",
    "$$L(D(G(z)), 0) = 0.\\log D(G(z)) + (1 - 0).\\log(1 - D(G(z)))$$\n",
    "$$L(D(G(z)), 0) = \\log(1- D(G(z)))$$\n",
    "\n",
    "Vagyis a $\\log(1- D(G(z)))$-t kell maximalizálnia\n",
    "\n",
    "Egyetlen mintára a hibafüggvény a következőképpen néz ki:\n",
    "\n",
    "$$\\max V(D) = \\log D(x) + \\log(1 - D(G(z))$$\n",
    "\n",
    "\n",
    "Batch-ra nézve:\n",
    "\n",
    "$$\\max V(D) = \\mathbb{E}_{x \\sim P(x))} \\left[\\log D(x) \\right] + \\mathbb{E}_{z \\sim P(z))} \\left[\\log(1 - D(G(z))) \\right]$$\n",
    "\n",
    "Ahol a $P(x)$ a valószínűségi eloszlása a tanítóhalmaznak, $P(z)$ a valószínűségi eloszlása a $z$ zajvektornak. (látens tér).\n",
    "\n",
    "##### Generátor hibafüggvénye\n",
    "A _Generátor_ feladata az, hogy megtévessze a _Diszkriminátort_ azáltal, hogy a tanítóhalmazhoz hasonló adatokat generáljon.\n",
    "Vagyis a _Generátor_ érdeke az, hogy a $D(G(z))$ 1-es címkét kapjon a _Diszkriminátortól_ 0 helyett.\n",
    "\n",
    "Tehát a bináris keresztentrópia egy mintára:\n",
    "$$L(D(G(z)), 0) = \\log(1 - D(G(z))$$\n",
    "\n",
    "Tehát a _Diszkriminátor_ minimalizálni kívánja a $D(G(z))$-t, míg a _Generátor_ maximalizálni szándékozik azt.\n",
    "\n",
    "A _Generátor_ a tanítás során sosem fog valódi adatot látni, de a teljesség kedvééert a hibafüggvénye a következőképpen írható fel (Csak a második kifejezést minimalizálja valójában):\n",
    "\n",
    "$$\\min V(G) = \\mathbb{E}_{x \\sim P(x))} \\left[\\log D(x) \\right] + \\mathbb{E}_{z \\sim P(z))} \\left[\\log(1 - D(G(z))) \\right]$$\n",
    "\n",
    "\n",
    "Vagyis a GAN hálózat tanítása során $D$ és $G$ egy minimax játékot játszanak a $V(G, D)$ értékfüggvénnyel.\n",
    "\n",
    "$$\\min_{G}\\max_{D}V(D, G) =  \\mathbb{E}_{x \\sim P(x))} \\left[\\log D(x) \\right] + \\mathbb{E}_{z \\sim P(z))} \\left[\\log(1 - D(G(z))) \\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bcef25",
   "metadata": {},
   "source": [
    "#### Optimalizáló módszer\n",
    "A Generátorban is és a Diszkriminátoban is az Adam optimalizálót használtam. (Kezdeti paraméterek, learning rate, beta1-2?)\n",
    "\n",
    "#### Tanítási lépés\n",
    "%% GAN 2014 cikkből\n",
    "\n",
    "A GAN hálózat egy tanítási lépése a következő lépésekből áll:\n",
    "\n",
    "Legyen $m$ a minibatch elemszáma $m \\in \\mathbb{N}$\n",
    "\n",
    "1. Hozzunk létre $m$ darab zajmintát $(z_1, \\ldots, z_m)$ standard-normális eloszlásból $P_g(z)$.\n",
    "2. A tanítóhalmazból emeljük ki a soronkövetkező $m$ darab tanítómintát (képet), és ezt jelöljük $(x_1, \\ldots, x_m)$-el $P_{\\text{data}}(x)$\n",
    "3. Frissítsük a _Diszkriminátort_ a sztochasztikus gradiens emelkedésével (?? tükörfordítás)\n",
    "$$ \\nabla \\theta_d \\frac{1}{m} \\sum_{i=1}^{m} \\left[\\log D(x_i) + \\log(1 - D(G(z_i))) \\right]$$\n",
    "4. Frissítsük a _Generátort_ a sztochasztikus gradiens lejtésével (?? tükörfordítás)\n",
    "$$ \\nabla \\theta_d \\frac{1}{m} \\sum_{i=1}^{m} \\log(1 - D(G(z_i)))$$\n",
    "\n",
    "\n",
    "Természetesen nem egyszerre tanítjuk a GAN hálózat részeit. Az eredeti cikkben is javaslatot tesznek arra, hogy a _Diszkriminátort_ esetleg több lépésben is lehetne tanítani, majd a _Generátort_ egyetlen lépésben frissíteni.\n",
    "Különböző tanítási stratégiákban ez is egy szabad paraméter lehet. Számomra megfelelő volt az 1:1-es tanítási lépés alkalmazása is... (Esetleg lehetne mérni valahogy, hogy van-e számottevő különbség...)\n",
    "A tanítás hossza természetesen függ az adathalmaz méretétől, a batch mérettől, a modellben található paraméterektől és az optimalizáló függvénytől.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca58bb4",
   "metadata": {},
   "source": [
    "## GAN teljesítményének mérese, technikák\n",
    "- Inception Score (salimans2016improved, barratt2018note)\n",
    "- Fréchet Inception Distance (heusel2017gans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b47d75",
   "metadata": {},
   "source": [
    "## Data augmentation - tanítás kevés adattal\n",
    "- karras2020training\n",
    "- noguchi2019image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb4a3c",
   "metadata": {},
   "source": [
    "## Regularizációs módszerek\n",
    "(Még csak a Batch Normalization-t próbáltam az alapokon kívül)\n",
    "\n",
    "A GAN tanítása során nehézségekbe ütközhetünk. Legrosszabb esetben nem is kezd el konvergálni a generátor kimenete a tanítóminta képeihez, viszont abban az esetben gyanakodhatunk, hogy a modellünk nem lett helyesen felépítve. Amennyiben mégis elkezd fejlődni a modell és nem csak zaj jelenik meg a kimeneten az egyes tanítólépések után, de bizonyos számú epoch után mode collapse lép fel, úgy fontolóra vehetjük a következő regularizációs technikákat.\n",
    "\n",
    "### Megfelelő inicializációs stratégia\n",
    "\n",
    "### Ativációs függvények és  paramétereiknek a helyes megválasztása\n",
    "\n",
    "### Batch normalization\n",
    "\n",
    "### Spectral Normalization\n",
    "\n",
    "### Experience replay\n",
    "\n",
    "### Mini-batch discrimination"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
