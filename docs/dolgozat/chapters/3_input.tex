\Chapter{Bemenet feldolgozása}

Az inverz probléma bemeneteként az eredeti, objektumfelismerési probléma kimeneteire lesz szükségünk. A Koncepció fejezeteben az objektumfelismerés áttekintésében említésre kerültek a lehetséges kimenetek. Ha osztályozási feladatot értünk az objektumfelismerés alatt, akkor a felismerő modell kimenetén az egyes osztályok valószínűségi értékei figyelhetők meg. Az általam elkészített implementáció is valójában ilyen adatokat vár, majd ezen adatok szerint hajtja végre a képek kigenerálását.

Ezen fejezet a \ref{fig:architecture} ábrán megfigyelhető \texttt{Bemeneti szöveg} és a \texttt{Strukturált adat} részeivel foglalkozik. Vagyis a bemeneti adatok feldolgozásával, amelyek a további komponensek működéséhez szükségesek.

Az osztályokra vonatkozó adatokat a modell természetes nyelvű szöveg formájában kapja meg. A bemeneti szöveg ezután az általam megadott szabályok szerint strukturált adattá alakul, amelyben a különféle osztályok súlyozásai szerepelnek.

A képek generálást elvégző modelljeim olyan adathalmazokra tanultak, amelyekben meghatározott számú osztály található. Viszont például a Cifar-10 esetén az osztályok csupán egész számokkal vannak jelölve 0-10-ig és így minden esetben ki kell keresni, hogy melyik szám mit jelent. Tehát ehhez az adathalmazhoz nincsen konkrét szöveges adat, csak a publikált táblázatból olvashatók ki a az osztályokhoz tartozó címkék. Egy olyan megoldást kívánok bemutatni, amely nem szorul gépi tanulásos módszerekre és egyszerű szinonima szótár segítségével detektálja a mondatokban szereplő objektumokat.

\Section{Természetes nyelvi szöveg feldolgozása}

A bemeneti szöveget magyar nyelven várja a program. A szövegnek egy olyan leíró mondatnak kell lennie, amely a kigenerálandó kép tartalmát írja le, vagyis, hogy mi szerepel a képen. Mivel a képek generálásáért felelős komponensem csupán osztályokat tud kigenerálni, így a képen egy-egy objektum figyelhető meg csupán. Eszerint csak olyan bemeneti mondatokat tud értelmezni a program, amely az általa ismert osztályokra vonatkozik. Viszont lehetőség van kevert osztályok megadására is, különféle súlyokkal, amit kétféleképpen lehet megadni. Ha a bemeneti mondatot olyan kifejezésekkel látjuk el, amelyben több osztály is megfigyelhető, úgy a mondat feldolgozása után kapott strukturált adatban az említett osztályok hasonló súlyozással fognak szerepelni. Egy másik lehetőségként százalékos értékekkel is elláthatjuk a kigenerálandó objektumokat. A kigenerált képen továbbra is egy objektum figyelhető majd meg, viszont olyan kép kerül előállításra, amely az említett osztályok tulajdonságait hordozza a megadott súlyozás szerint.

Valójában a bemenet ilyen formában várása csupán egy felhasználó élményt növelő funkció. Az interakció ilyen formája minden bizonnyal természetesebbnek hathat a felhasználói oldalon. Ha egy olyan képgeneráló modellel kívánunk dolgozni, amely több osztályt ismer, akkor a bemenet ilyen formában történő bevitele tűnhet a legegyszerűbbnek a felhasználó számára.

A bemeneti mondat egy egyszerű normalizálási folyamaton megy keresztül, hogy a további feldolgozást megkönnyítse. A normalizálási folyamat a következő:

\begin{enumerate}
	\item Írásjelek eltávolítása
	\item A szöveg kisbetűssé tétele
	\item A szavak kigyűjtése (tokenek megalkotása)
	\item A százalékos értékeket jelző tokenek szerializálása
\end{enumerate}

Az utolsó pont csupán egy olyan műveletet jelöl, amely során a százalékos értékeket képviselő tokeneket egy egységesített formában hozza, hogy a következő lépésben könnyebben lehessen kezelni ezen értékeket. Ha a tokenizálás során az érték és a százalék szimbólum két külön tokenbe került, akkor ilyen esetben a két token egységesítődik és egy tokenként él tovább. Ha esetleg a bemenetben az érték után a "százalék" szóval került megadásra a $\%$ szimbólum, úgy a "százalék" szó törlésre kerül és egységesen a $\%$ szimbólum kerül az értékek mögé.

Ezen szabályok természetesen elég egyszerűek és specifikusak és csupán a már említett strukturált adatok megalkotásához szükségesek a bemeneti mondatok.

További normalizáló lépés is létezhet. Például a szavak toldalékainak leválasztása (\textit{stemming}) vagy a jelentéshez nem feltétlenül szükséges kötőszavak kiszűrése is használható a normalizálásban \cite{bird2009natural}. Mivel a bemeneti mondatok elég specifikusak és kicsit kötöttek is, így további normalizálás nem lett alkalmazva a bemenetre a felsoroltakon kívül.

\Section{Strukturált adatok megalkotása}

Ha a bemeneti mondat normalizálásra került, úgy előállítható belőle a strukturált adat, amely jelen esetben \textit{python dictionary} formátumban kerül megalkotásra. A strukturált adatok kinyerése a szövegből egyik technikája a \textit{Named Entity Recognition} \cite{bird2009natural}, amely az egyes tokeneket osztályozza a szövegben elfoglalt helyük és szófajok figyelembevételével. A dolgozatom ezen része nem terjed ki olyan részletesen a Természetes Nyelvi Szövegfeldolgozásra, a rendelkezésre álló adatok hiányában egy egyszerűbb megoldást választottam, amely szinonima szótár segítségével osztályozza a megfelelő tokeneket.

Az egyes modellekhez használatos szinonima szótár megalkotásához segítségemre volt az interneten talált szinonimaszotar.hu honlap \footnote{Magyar internetes szinonimaszótár \url{https://szinonimaszotar.hu}.}. Ezen példákhoz elegendőnek éreztem a szinonimák használatát is, viszont több osztályt ismerő modellek esetén a megoldásom valószínűleg már kevésbé lesz pontos, hiszen csupán a nyers szavakat hasonlítja össze betű szinten. Egy kifinomultabb megközelítés a szófajok, ragozás és mondatszerkezetek vizsgálata lehet. Vagy egy olyan \textit{embedding} megalkotása, amely az egymáshoz hasonló jelentésű szavakat közelebb helyezi el a belső terében, mint például a \textit{Word2Vec} \cite{mikolov2013efficient} módszer esetében, amely jelenleg csupán csak behivatkozásra kerül, ugyanis nem tettem ilyen jellegű vizsgálatot a dolgozatom során.

A mondat szavain egyesével megy végig az algoritmus, és összehasonlításra kerülnek a szavak az elkészített szinonimaszótár elemeivel. Amennyiben egy szinonima megtalálható a vizsgált szóban egy bizonyos mennyiségben, úgy a szót a szinonimához tartozó osztállyal látjuk el. A tartalmazás vizsgálata százalékos módon kerül kiértékelésre és a vizsgált szinonima pedig a szóban bárhol elhelyezkedhet, a betűk egyezése számít csupán. A 80\%-os egyezés vizsgálata megfelelőnek tűnt a példák futtatása során. Az összetett szavak vagy a prefixekkel, esetleg a ragokkal ellátott szavak alapjaiban az eredeti szó megfigyelhető, így elegendőnek éreztem ilyen szintű egyezés vizsgálatát is. Természetesen a jelentésbeli vizsgálat nem történt, így például a "ló" szó a "halló" szóban 100\%-os tartalmazást fog mutatni, annak ellenére, hogy egészen mást jelentenek a szavak.
Ezen módszer kevés osztályok mellett és megfelelő bemenetekre használható eredményeket szolgáltat, viszont a saját tudásbázisán kívül már nem képes megfelelően eredményeket adni.

A százalékos értékek az előző feldolgozási lépések alapján kerülnek detektálásra és feldolgozásra. Amennyiben az előző lépésben az osztályok megtalálásra kerültek és ha előttük található százalékos érték, akkor a strukturált adatban a megfelelő értékekkel kerülnek bejegyzésre. Ha nem minden detektált osztályhoz található százalékos érték, úgy a maradék értékek az említett osztályok között kiosztásra kerülnek.
Amennyiben megtalált valószínűségi értékek összege nem egyre jön ki, úgy a maradék osztályok között kerülnek szétosztásra az értékek. Ha a bemeneti mondatok nem tartalmaznak százalékos értékeket, úgy a mondatokban azonosított osztályok azonos súlyozással kerülnek rögzítésre.

A következő három példán megfigyelhető, hogy az egyes bemeneti mondatokra milyen strukturált adat készül el. Az első két példa az AFHQ-, míg az utolsó a CIFAR-10 osztályaira lett alkalmazva.

\noindent\textit{Egy vadkutya portréja található a képen.}
\small{
\begin{verbatim}
{
    'cat': 0.0,
    'dog': 0.5,
    'wild': 0.5
}
\end{verbatim}
}

\noindent\textit{75 \% macska van a képen, 25 százalék pedig öleb.}
\small{
\begin{verbatim}
{
    'cat': 0.75,
    'dog': 0.25,
    'wild': 0.0
}
\end{verbatim}
}

\noindent\textit{Fotó egy olyan lóról amely 30\%-ban agancsos.}
\small{
\begin{verbatim}
{
    'airplane': 0.0,
    'automobile': 0.0,
    'bird': 0.0,
    'cat': 0.0,
    'deer': 0.3,
    'dog': 0.0,
    'frog': 0.0,
    'horse': 0.7,
    'ship': 0.0,
    'truck': 0.0
}
\end{verbatim}
}

A példákban található egyéb kifejezéseket nem veszi figyelembe az algoritmus. Csupán az általa ismert osztályokra vonatkozó információk kerülnek kinyerésre, mivel a képet előállító komponens is csak ezen adatok alapján tud képet generálni. Egy fejlettebb modell esetében vizsgálatra kerülhetnének a számosságot, helyzetet jelölő szavak is például. Vagy az adott objektum egyes tulajdonságait leíró kifejezések, mint például a színre vagy textúrára vonatkozó megkötések is. Viszont a dolgozatban ezek sem kerültek vizsgálatra.
